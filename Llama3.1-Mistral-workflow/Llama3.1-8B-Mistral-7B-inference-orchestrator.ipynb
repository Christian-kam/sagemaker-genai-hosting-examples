{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fda7cf11",
   "metadata": {},
   "source": [
    "# Create and Deploy Custom Inference Workflows using the SageMaker PythonSDK."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2edda912",
   "metadata": {},
   "source": [
    "In this notebook you will learn how to use the `ModelBuilder` class to define and deploy your own custom inference workflows directly in the SageMaker PythonSDK and have them ready for serving. \n",
    "\n",
    "You will be able to define the `ResourceRequirements` for multiple Inference Components and deploy them in bulk. For a supported subset of Jumpstart models including the Llama3 family, you don't need to specify `ResourceRequirements` at all and can instead use pre-benchmarked deployment configs.\n",
    "\n",
    "You can either launch this notebook from an Amazon SageMaker studio notebook instance which handles all credentials automatically, or by running it locally and setting credentials manually. (Please make sure you are running on the latest studio image version >=3.0.0).\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2073801",
   "metadata": {},
   "source": [
    "### Additional Resources\n",
    "- To learn more about `ModelBuilder`, see [Create a model in Amazon SageMaker with ModelBuilder](https://docs.aws.amazon.com/sagemaker/latest/dg/how-it-works-modelbuilder-creation.html)\n",
    "- To learn more about Inference Components, see the [Inference Component Launch Blog](https://aws.amazon.com/blogs/aws/amazon-sagemaker-adds-new-inference-capabilities-to-help-reduce-foundation-model-deployment-costs-and-latency/)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eefc7c84",
   "metadata": {},
   "source": [
    "## Prerequisites\n",
    "This notebook was tested locally, as well as in SageMaker Studio using region `us-west-2` and `us-east-1` on the following platforms and configs.\n",
    "| Platform |Config Name | Value |\n",
    "| :--------  | :----------- | :---------- |\n",
    "| JupyterLab |Instance Type| ml.m5.xlarge |\n",
    "|            |Image         | SageMaker Distribution 2.0.0 |\n",
    "|            |Kernel        | Python 3 (ipykernel) |\n",
    "|            |Python Version| Python 3.12|\n",
    "| Studio Classic |Instance Type| ml.t3.medium |\n",
    "|            |Image         | Data Science 4.0 |\n",
    "|            |Kernel        | Python 3 |\n",
    "|            |Python Version| Python 3.12|\n",
    "\n",
    "\n",
    "For best performance, use Python 3.12. \n",
    "\n",
    "Note that, To enable live-logging when deploying your custom orchestrator, ensure your execution role has `logs:FilterLogEvents` permissions. The default notebook role does NOT have this by default."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2b63326-c8aa-4605-9e4d-74ccc673d008",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%pip install --upgrade pip\n",
    "%pip install --upgrade sagemaker"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26b1b07d",
   "metadata": {},
   "source": [
    "### Step 1: Create our Inference Component `ModelBuilder` objects.\n",
    "Define a `ModelBuilder` object for each Inference Component you'd like to create. Set either `inference_component_name` or `resource_requirements` to signify an IC should be created. \n",
    "\n",
    "Note: Some JumpStart models contain pre-benchmarked `ResourceRequirements` deployment configurations, so all you need to do is set an `inference_component_name`. \n",
    "This notebook uses `\"meta-textgeneration-llama-3-1-8b\"` to show off using pre-benchmarked deployment configurations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebb462d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import uuid\n",
    "import boto3\n",
    "from sagemaker import get_execution_role\n",
    "\n",
    "# Get the SageMaker execution role\n",
    "role = get_execution_role()\n",
    "\n",
    "# Define the names for our inference components and endpoint.\n",
    "llama_mistral_endpoint_name = f\"llama-mistral-endpoint-{uuid.uuid1().hex}\"\n",
    "mistral_ic_name = f\"mistral-ic-{uuid.uuid1().hex}\"\n",
    "llama_ic_name = f\"llama-ic-{uuid.uuid1().hex}\"\n",
    "\n",
    "region = boto3.Session().region_name"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9de6f31e-7edd-4571-ba02-515ea309ca59",
   "metadata": {},
   "source": [
    "We first set up a ModelBuilder for deploying Llama 3.1 8B model with specific configurations:\n",
    "\n",
    "1. Model Selection: Uses the Llama 3.1 8B text generation model\n",
    "2. Schema Definition:\n",
    "    - Input schema includes the prompt text and generation parameters\n",
    "    - Output schema defines the expected response format\n",
    "3. Resource Requirements: Define the resources required to deploy the model inference component. Here we will deploy a single copy of the model\n",
    "\n",
    "Note that in this notebook we use the default SageMaker execution role, you can follow the [least-privilege permissions](https://docs.aws.amazon.com/sagemaker/latest/dg/security_iam_id-based-policy-examples.html) to manage resources created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee6e4f04",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sagemaker.session import Session\n",
    "from sagemaker.serve import ModelBuilder\n",
    "from sagemaker.serve.builder.schema_builder import SchemaBuilder\n",
    "from sagemaker.compute_resource_requirements.resource_requirements import ResourceRequirements\n",
    "\n",
    "\n",
    "# Define sample input and output for the model\n",
    "prompt = \"Falcons are\"\n",
    "response = \"Falcons are small to medium-sized birds of prey related to hawks and eagles.\"\n",
    "\n",
    "# Create the input schema structure\n",
    "sample_input = {\n",
    "    \"inputs\": prompt,\n",
    "    \"parameters\": {\"max_new_tokens\": 32}\n",
    "}\n",
    "# Define the expected output format\n",
    "sample_output = [{\"generated_text\": response}]\n",
    "\n",
    "# Create a ModelBuilder instance for Llama 3.1 8B\n",
    "# Pre-benchmarked ResourceRequirements will be taken from JumpStart, as Llama-3.1-8b is a supported model.\n",
    "llama_model_builder = ModelBuilder(\n",
    "    model=\"meta-textgeneration-llama-3-1-8b\",\n",
    "    schema_builder=SchemaBuilder(sample_input, sample_output),\n",
    "    inference_component_name=llama_ic_name,\n",
    "    instance_type=\"ml.g5.24xlarge\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00ccd7d6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "mistral_mb = ModelBuilder(\n",
    "    model=\"huggingface-llm-mistral-7b\",\n",
    "    schema_builder=SchemaBuilder(sample_input, sample_output),\n",
    "    inference_component_name=mistral_ic_name,\n",
    "    resource_requirements=ResourceRequirements(\n",
    "        requests={\n",
    "           \"memory\": 49152,\n",
    "           \"num_accelerators\": 2,\n",
    "           \"copies\": 1\n",
    "        }\n",
    "    ),\n",
    "    instance_type=\"ml.g5.24xlarge\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1c181b5",
   "metadata": {},
   "source": [
    "### Step 2: Define your custom inference orchestrator by creating a class which inherits from the new `CustomOrchestrator` class.\n",
    "`CustomOrchestrator` expects a single `handle()` function to be implemented, which will serve as the container entrypoint when your endpoint is invoked. This example uses vanilla Python and boto3.\n",
    "\n",
    "The following example is merely meant to demonstratge chaining together multiple SageMaker models, and does not necessarily reflect practical real-world use cases.\n",
    "\n",
    "\n",
    "```\n",
    "class CustomOrchestrator(ABC):\n",
    "    \"\"\"\n",
    "    Templated class used to standardize the structure of an entry point based inference script.\n",
    "    \"\"\"\n",
    "\n",
    "    @abstractmethod\n",
    "    def handle(self, data, context=None):\n",
    "        \"\"\"abstract class for defining an entrypoint for the model server\"\"\"\n",
    "        return NotImplemented\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "338ea2f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.serve.spec.inference_base import CustomOrchestrator"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dbcb9c6-4759-4c50-ae6d-956a4397109a",
   "metadata": {},
   "source": [
    "This implementation creates a pipeline where the output of the Llama model becomes the input for the Mistral model, allowing for sequential processing of text through two different language models within the same endpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6aa8e61f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "class PythonCustomInferenceEntryPoint(CustomOrchestrator):\n",
    "    def __init__(self, region_name, endpoint_name, component_names):\n",
    "        self.region_name = region_name\n",
    "        self.endpoint_name = endpoint_name\n",
    "        self.component_names = component_names\n",
    "    \n",
    "    def preprocess(self, data):\n",
    "        payload = {\n",
    "            \"inputs\": data.decode(\"utf-8\")\n",
    "        }\n",
    "        return json.dumps(payload)\n",
    "\n",
    "    def _invoke_workflow(self, data):\n",
    "        # First model (Llama) inference\n",
    "        payload = self.preprocess(data)\n",
    "        \n",
    "        llama_response = self.client.invoke_endpoint(\n",
    "            EndpointName=self.endpoint_name,\n",
    "            Body=payload,\n",
    "            ContentType=\"application/json\",\n",
    "            InferenceComponentName=self.component_names[0]\n",
    "        )\n",
    "        llama_generated_text = json.loads(llama_response.get('Body').read())['generated_text']\n",
    "        \n",
    "        # Second model (Mistral) inference\n",
    "        parameters = {\n",
    "            \"max_new_tokens\": 50\n",
    "        }\n",
    "        payload = {\n",
    "            \"inputs\": llama_generated_text,\n",
    "            \"parameters\": parameters\n",
    "        }\n",
    "        mistral_response = self.client.invoke_endpoint(\n",
    "            EndpointName=self.endpoint_name,\n",
    "            Body=json.dumps(payload),\n",
    "            ContentType=\"application/json\",\n",
    "            InferenceComponentName=self.component_names[1]\n",
    "        )\n",
    "        return {\"generated_text\": json.loads(mistral_response.get('Body').read())['generated_text']}\n",
    "    \n",
    "    def handle(self, data, context=None):\n",
    "        return self._invoke_workflow(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "561a4f29",
   "metadata": {},
   "source": [
    "### Step 3: Create a `ModelBuilder` (which represents the custom orchestrator) and pass it in via the `inference_spec` field like so.\n",
    "`ModelBuilder` will know to deploy as a custom orchestrator if `inference_spec` contains an instance of `CustomOrchestrator`.\n",
    "\n",
    "\n",
    "`ModelBuilder` will be able to automatically capture module-level dependencies if \"auto\" is set to True. If you have any dependencies declared elsewhere, set \"auto\" to False and list whatever packages you may need.\n",
    "\n",
    "Then pass in the Inference Component ModelBuilder objects we just created.\n",
    "\n",
    "- `modelbuilder_list` - `ModelBuilder` objects we just created for each Inference Component and the custom orchestrator.\n",
    "\n",
    "Calling `build()` will prepare the chain for deployment, which can be triggered via `deploy()`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54425620",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sagemaker.serve.builder.model_builder import ModelBuilder, SchemaBuilder\n",
    "from sagemaker.session import Session\n",
    "from sagemaker.resource_requirements import ResourceRequirements\n",
    "\n",
    "custom_orchestrator_name = f\"custom-orchestrator-{uuid.uuid1().hex}\"\n",
    "\n",
    "orchestrator = ModelBuilder(\n",
    "    inference_spec=PythonCustomInferenceEntryPoint(\n",
    "        region_name=region,\n",
    "        endpoint_name=llama_mistral_endpoint_name,\n",
    "        component_names=[llama_ic_name, mistral_ic_name],\n",
    "    ),\n",
    "    dependencies={\n",
    "        \"auto\": False,\n",
    "        \"custom\": [\n",
    "            \"cloudpickle\",\n",
    "            \"graphene\",\n",
    "            # Define other dependencies here.\n",
    "        ],\n",
    "    },\n",
    "    sagemaker_session=Session(),\n",
    "    role_arn=role,\n",
    "    resource_requirements=ResourceRequirements(\n",
    "        requests={\n",
    "           \"memory\": 4096,\n",
    "           \"num_accelerators\": 1,\n",
    "           \"copies\": 1,\n",
    "           \"num_cpus\": 2\n",
    "        }\n",
    "    ),\n",
    "    inference_component_name=custom_orchestrator_name, # IC name for your custom orchestrator\n",
    "    name=custom_orchestrator_name, # Endpoint name if you want the custom orchestrator on its own endpoint\n",
    "    schema_builder=SchemaBuilder(sample_input=\"Test\", sample_output={\"generated_text\": \"test\"}),\n",
    "    modelbuilder_list=[llama_model_builder, mistral_mb] # Inference Component ModelBuilders created in Step 2\n",
    ")\n",
    "\n",
    "# call the build function to prepare the chain for deployment\n",
    "orchestrator.build()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb3d3cc1",
   "metadata": {},
   "source": [
    "Calling `deploy()` will deploy your inference component ModelBuilders to your desired instance type, and your custom workflow to 1 instance of `ml.c5.xlarge` by default.\n",
    "You can set `custom_orchestrator_instance_type` and `custom_orchestrator_initial_instance_count` to configure these values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89e78458-4e80-4f60-86a4-df696e7a343a",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "predictors = orchestrator.deploy(\n",
    "    instance_type=\"ml.g5.24xlarge\",\n",
    "    initial_instance_count=1,\n",
    "    accept_eula=True, # Required for Llama3\n",
    "    endpoint_name=llama_mistral_endpoint_name,\n",
    "    # custom_orchestrator_instance_type=\"ml.t2.medium\", # default\n",
    "    # custom_orchestrator_initial_instance_count=1 # default\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84f0d211",
   "metadata": {},
   "source": [
    "### Test Invoking the Inference Components and Custom Orchestrator Endpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7690099",
   "metadata": {},
   "source": [
    "Test the custom orchestrator. \n",
    "If your orchestrator supports streaming, you can call `Predictor.predict_stream()` with `\"stream\": True` set in the payload body to get a streaming response.\n",
    "```\n",
    "\n",
    "generator = predictor.predict_stream(\n",
    "    json.dumps({\n",
    "        \"prompt\": \"where is the capital of india?\",\n",
    "        \"stream\": True\n",
    "    })\n",
    ")\n",
    "\n",
    "for chunk in generator:\n",
    "    print(\n",
    "        str(chunk, encoding = 'utf-8'), \n",
    "        end = \"\", \n",
    "        flush = True\n",
    "    )\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5036ffc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.serializers import JSONSerializer\n",
    "predictors[-1].serializer = JSONSerializer()\n",
    "predictors[-1].predict(\"Tell me a story about ducks.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "856ae148",
   "metadata": {},
   "source": [
    "Let's just verify the Inference Components work on their own. We'll test the Llama IC with a synchronous invocation, and Mistral with streaming."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13c9df25",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sagemaker.predictor import Predictor\n",
    "mistral_predictor = Predictor(endpoint_name=llama_mistral_endpoint_name, component_name=mistral_ic_name)\n",
    "mistral_predictor.content_type = \"application/json\"\n",
    "llama_predictor = Predictor(endpoint_name=llama_mistral_endpoint_name, component_name=llama_ic_name)\n",
    "llama_predictor.content_type = \"application/json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "668e6321",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import json\n",
    "payload = {\n",
    "    \"inputs\": \"What is the capital of Japan?\"\n",
    "}\n",
    "\n",
    "llama_predictor.predict(json.dumps(payload))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21799a36",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import boto3\n",
    "import json\n",
    "\n",
    "# Define the prompt and other parameters\n",
    "prompt = \"\"\"\n",
    "<s>[INST] Below is the question based on the context. \n",
    "Question: Given a reference text about Lollapalooza, where does it take place, who started it and what is it?. \n",
    "Below is the given the context Lollapalooza /ˌlɒləpəˈluːzə/ (Lolla) is an annual American four-day music festival held in Grant Park in Chicago. \n",
    "It originally started as a touring event in 1991, but several years later, Chicago became its permanent location. Music genres include but are not limited to alternative rock, heavy metal, punk rock, hip hop, and electronic dance music. Lollapalooza has also featured visual arts, nonprofit organizations, and political organizations. \n",
    "The festival, held in Grant Park, hosts an estimated 400,000 people each July and sells out annually. Lollapalooza is one of the largest and most iconic music festivals in the world and one of the longest-running in the United States. Lollapalooza was conceived and created in 1991 as a farewell tour by Perry Farrell, singer of the group Jane's Addiction.. \n",
    "Write a response that appropriately completes the request.[/INST]\n",
    "\"\"\"\n",
    " \n",
    "max_tokens_to_sample = 200\n",
    "\n",
    "# hyperparameters for llm\n",
    "parameters = {\n",
    "    \"max_new_tokens\": max_tokens_to_sample,\n",
    "    \"do_sample\": True,\n",
    "    \"top_p\": 0.9,\n",
    "    \"temperature\": 0.5,\n",
    "}\n",
    "\n",
    "contentType = 'application/json'\n",
    "\n",
    "body = json.dumps({\n",
    "    \"inputs\": prompt,\n",
    "    # specify the parameters as needed\n",
    "    \"parameters\": parameters\n",
    "})\n",
    "\n",
    "mistral_predictor.content_type = contentType\n",
    "for line in mistral_predictor.predict_stream(body):\n",
    "    decoded_line = line.decode('utf-8')\n",
    "    if '\\n' in decoded_line:\n",
    "        # Split by newline to handle multiple tokens in the same line\n",
    "        tokens = decoded_line.split('\\n')\n",
    "        for token in tokens[:-1]:  # Print all tokens except the last one with a newline\n",
    "            print(token)\n",
    "        # Print the last token without a newline, as it might be followed by more tokens\n",
    "        print(tokens[-1], end='')\n",
    "    else:\n",
    "        # Print the token without a newline if it doesn't contain '\\n'\n",
    "        print(decoded_line, end='')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1afc104",
   "metadata": {},
   "source": [
    "### Cleanup the Resources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31b8cedf-c0d3-484b-9370-586faa57bf71",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictors[-1].delete_predictor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a571380",
   "metadata": {},
   "outputs": [],
   "source": [
    "mistral_predictor.delete_predictor()\n",
    "llama_predictor.delete_predictor()\n",
    "llama_predictor.delete_endpoint()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "992629ec-0fac-4e28-b4ff-ed8ac9fbe17c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
