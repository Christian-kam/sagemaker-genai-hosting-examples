{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "27ddeb2c-7877-46e3-9a4e-b2efb0d1b7a4",
   "metadata": {},
   "source": [
    "# How to deploy the Gemma 3 27B instruct for inference using Amazon SageMakerAI with LMI v15 powered by vLLM 0.8.4\n",
    "**Recommended kernel(s):** This notebook can be run with any Amazon SageMaker Studio kernel.\n",
    "\n",
    "In this notebook, you will learn how to deploy the Gemma 3 27 B instruct model (HuggingFace model ID: [google/gemma-3-27b-it](https://huggingface.co/google/gemma-3-27b-it)) using Amazon SageMaker AI. The inference image will be the SageMaker-managed [LMI (Large Model Inference) v15 powered by vLLM 0.8.4](https://docs.aws.amazon.com/sagemaker/latest/dg/large-model-inference-container-docs.html) Docker image. LMI images features a [DJL serving](https://github.com/deepjavalibrary/djl-serving) stack powered by the [Deep Java Library](https://djl.ai/). \n",
    "\n",
    "Gemma 3 models are multimodal, handling text and image input and generating text output, with open weights for both pre-trained variants and instruction-tuned variants. Gemma 3 has a large, 128K context window, multilingual support in over 140 languages, and is available in more sizes than previous versions. Gemma 3 models are well-suited for a variety of text generation and image understanding tasks, including question answering, summarization, and reasoning. Their relatively small size makes it possible to deploy them in environments with limited resources such as laptops, desktops or your own cloud infrastructure, democratizing access to state of the art AI models and helping foster innovation for everyone.\n",
    "\n",
    "### License agreement\n",
    "* This model is gated on HuggingFace, please refer to the original [model card](https://huggingface.co/google/gemma-3-27b-it) for license.\n",
    "* This notebook is a sample notebook and not intended for production use.\n",
    "\n",
    "### Execution environment setup\n",
    "This notebook requires the following third-party Python dependencies:\n",
    "* AWS [`sagemaker`](https://sagemaker.readthedocs.io/en/stable/index.html) with a version greater than or equal to 2.242.0\n",
    "\n",
    "Let's install or upgrade these dependencies using the following command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a645403-0c3e-4062-9d16-ef0b1041fbe3",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "%pip install -Uq sagemaker"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa5631d3-1c16-4ad5-a42c-85a28cf9dd3e",
   "metadata": {},
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65310881-31a9-453e-9f7b-c79876824cd8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "import boto3\n",
    "import logging\n",
    "import time\n",
    "from sagemaker.session import Session\n",
    "from sagemaker.s3 import S3Uploader\n",
    "\n",
    "print(sagemaker.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83937110-ffc0-4c42-b67d-0021b829f25a",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    role = sagemaker.get_execution_role()\n",
    "    sagemaker_session  = sagemaker.Session()\n",
    "    \n",
    "except ValueError:\n",
    "    iam = boto3.client('iam')\n",
    "    role = iam.get_role(RoleName='sagemaker_execution_role')['Role']['Arn']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9d3035e-f732-4429-a7a5-89bf8f822750",
   "metadata": {},
   "outputs": [],
   "source": [
    "HF_MODEL_ID = \"google/gemma-3-27b-it\"\n",
    "\n",
    "base_name = HF_MODEL_ID.split('/')[-1].replace('.', '-').lower()\n",
    "model_lineage = HF_MODEL_ID.split(\"/\")[0]\n",
    "base_name"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8d5d428-e250-47e8-b751-c48f38fd6b55",
   "metadata": {},
   "source": [
    "## Configure Model Serving Properties\n",
    "\n",
    "Now we'll create a `serving.properties` file that configures how the model will be served. This configuration is crucial for optimal performance and memory utilization.\n",
    "\n",
    "Key configurations explained:\n",
    "- **Engine**: Python backend for model serving\n",
    "- **Model Settings**:\n",
    "  -  Using gemma-3-27b-it \n",
    "  - Maximum sequence length of 32768 tokens\n",
    "  - model loading timeout of 1200 seconds (20 minutes)\n",
    "- **Performance Optimizations**:\n",
    "  - Tensor parallelism across all available GPUs\n",
    "  - Max rolling batch size of 16 for efficient batching\n",
    "  \n",
    "#### Understanding KV Cache and Context Window\n",
    "\n",
    "The `max_model_len` parameter controls the maximum sequence length the model can handle, which directly affects the size of the KV (Key-Value) cache in GPU memory.\n",
    "\n",
    "1. Start with a conservative value (current: 32768)\n",
    "2. Monitor GPU memory usage\n",
    "3. Incrementally increase if memory permits\n",
    "4. Target the model's full context window "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c753dfbe-803b-478a-8dd7-97c8928eaf6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the directory that will contain the configuration files\n",
    "from pathlib import Path\n",
    "\n",
    "model_dir = Path('config')\n",
    "model_dir.mkdir(exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d6ae607-c0a6-46f7-9ca1-df71893f6ed2",
   "metadata": {},
   "source": [
    "If you are deploying a model hosted on the HuggingFace Hub, you must specify the `option.model_id=<hf_hub_model_id>` configuration. When using a model directly from the hub, we recommend you also specify the model revision (commit hash or branch) via `option.revision=<commit hash/branch>`. \n",
    "\n",
    "Since model artifacts are downloaded at runtime from the Hub, using a specific revision ensures you are using a model compatible with package versions in the runtime environment. Open Source model artifacts on the hub are subject to change at any time. These changes may cause issues when instantiating the model (updated model artifacts may require a newer version of a dependency than what is bundled in the container). If a model provides custom model (modeling.py) and/or custom tokenizer (tokenizer.py) files, you need to specify option.trust_remote_code=true to load and use the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bb092ff-a52e-442f-890b-c7c6a7e3d08b",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = f\"\"\"engine=Python\n",
    "option.async_mode=true\n",
    "option.rolling_batch=disable\n",
    "option.entryPoint=djl_python.lmi_vllm.vllm_async_service\n",
    "option.tensor_parallel_degree=max\n",
    "option.model_loading_timeout=1200\n",
    "fail_fast=true\n",
    "option.max_model_len=32768\n",
    "option.max_rolling_batch_size=16\n",
    "option.trust_remote_code=true\n",
    "option.model_id={HF_MODEL_ID}\n",
    "option.revision=main\n",
    "\"\"\"\n",
    "\n",
    "with open(\"config/serving.properties\", \"w\") as f:\n",
    "    f.write(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0094ec7a-5409-4ba8-8b23-b99c4d4a3bae",
   "metadata": {},
   "source": [
    "**Best Practices**:\n",
    ">\n",
    "> **Store Models in Your Own S3 Bucket**\n",
    "For production use-cases, always download and store model files in your own S3 bucket to ensure validated artifacts. This provides verified provenance, improved access control, consistent availability, protection against upstream changes, and compliance with organizational security protocols.\n",
    ">\n",
    ">**Separate Configuration from Model Artifacts**\n",
    "> The LMI container supports separating configuration files from model artifacts. While you can store serving.properties with your model files, placing configurations in a distinct S3 location allows for better management of all your configurations files.\n",
    ">\n",
    "> When your model and configuration files are in different S3 locations, set `option.model_id=<s3_model_uri>` in your serving.properties file, where `s3_model_uri` is the S3 object prefix containing your model artifacts."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cff13de0-159d-4d77-95bd-362735c2ef08",
   "metadata": {},
   "source": [
    "#### Optional configuration files\n",
    "\n",
    "(Optional) You can also specify a `requirements.txt` to install additional libraries."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe6dd00a-d351-4825-a8e4-6e7629e1c1fc",
   "metadata": {},
   "source": [
    "### Upload config files to S3\n",
    "SageMaker AI allows us to provide [uncompressed](https://docs.aws.amazon.com/sagemaker/latest/dg/large-model-inference-uncompressed.html) files. Thus, we directly upload the folder that contains `serving.properties` to s3\n",
    "> **Note**: The default SageMaker bucket follows the naming pattern: `sagemaker-{region}-{account-id}`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f34b933-26ad-4017-9cda-a4450d90f905",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.s3 import S3Uploader\n",
    "\n",
    "sagemaker_default_bucket = sagemaker_session.default_bucket()\n",
    "\n",
    "config_files_uri = S3Uploader.upload(\n",
    "    local_path=\"config\",\n",
    "    desired_s3_uri=f\"s3://{sagemaker_default_bucket}/lmi/{base_name}/config-files\"\n",
    ")\n",
    "\n",
    "print(f\"code_model_uri: {config_files_uri}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c78d08f-407c-4c57-aa61-172fc28729f0",
   "metadata": {},
   "source": [
    "## Configure Model Container and Instance\n",
    "\n",
    "For deploying Gemma-3-27B-it, we'll use:\n",
    "- **LMI (Deep Java Library) Inference Container**: A container optimized for large language model inference\n",
    "- **[G6e Instance](https://aws.amazon.com/ec2/instance-types/g6e/)**: AWS's GPU instance type powered by NVIDIA L40S Tensor Core GPUs \n",
    "\n",
    "Key configurations:\n",
    "- The container URI points to the DJL inference container in ECR (Elastic Container Registry)\n",
    "- We use `ml.g6e.48xlarge` instance which offer:\n",
    "  - 8 NVIDIA L40S Tensor Core GPUs\n",
    "  - 384 GB of total GPU memory (48 GB of memory per GPU)\n",
    "  - up to 400 Gbps of network bandwidth\n",
    "  - up to 1.536 TB of system memory\n",
    "  - and up to 7.6 TB of local NVMe SSD storage.\n",
    "\n",
    "> **Note**: The region in the container URI should match your AWS region."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b16a3759-4cce-4a69-9f77-68251aabbb25",
   "metadata": {},
   "outputs": [],
   "source": [
    "gpu_instance_type = \"ml.g6e.48xlarge\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c013786-ac4e-4213-b4a0-29c851077aa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_uri = \"763104351884.dkr.ecr.{}.amazonaws.com/djl-inference:0.33.0-lmi15.0.0-cu128\".format(sagemaker_session.boto_session.region_name)\n",
    "print(image_uri)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4104f5e-883a-4ab3-a82e-93b3b85b43f4",
   "metadata": {},
   "source": [
    "## Create SageMaker Model\n",
    "\n",
    "Now we'll create a SageMaker Model object that combines our:\n",
    "- Container image (LMI)\n",
    "- Model artifacts (configuration files)\n",
    "- IAM role (for permissions)\n",
    "\n",
    "This step defines the model configuration but doesn't deploy it yet. The Model object represents the combination of:\n",
    "\n",
    "1. **Container Image** (`image_uri`): DJL Inference optimized for LLMs\n",
    "2. **Model Data** (`model_data`): Our configuration files in S3\n",
    "3. **IAM Role** (`role`): Permissions for model execution\n",
    "\n",
    "### Required Permissions\n",
    "The IAM role needs:\n",
    "- S3 read access for model artifacts\n",
    "- CloudWatch permissions for logging\n",
    "- ECR permissions to pull the container\n",
    "\n",
    "#### HUGGING_FACE_HUB_TOKEN \n",
    "Gemma-3-27B-Instruct is a gated model. Therefore, if you deploy model files hosted on the Hub, you need to provide your HuggingFace token as environment variable. This enables SageMaker AI to download the files at runtime."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09b4ca9e-142d-41cb-82a8-15820c7232e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the S3 URI for your uncompressed config files\n",
    "model_data = {\n",
    "    \"S3DataSource\": {\n",
    "        \"S3Uri\": f\"{config_files_uri}/\",\n",
    "        \"S3DataType\": \"S3Prefix\",\n",
    "        \"CompressionType\": \"None\"\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ea1bb53-f5f3-490e-aca9-a9790481798a",
   "metadata": {},
   "outputs": [],
   "source": [
    "HUGGING_FACE_HUB_TOKEN = \"hf_\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "527937f2-43e9-428a-b201-ce299894390d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.utils import name_from_base\n",
    "from sagemaker.model import Model\n",
    "\n",
    "model_name = name_from_base(base_name, short=True)\n",
    "\n",
    "# Create model\n",
    "gemma_3_model = Model(\n",
    "    name = model_name,\n",
    "    image_uri=image_uri,\n",
    "    model_data=model_data,  # Path to uncompressed code files\n",
    "    role=role,\n",
    "    env={\n",
    "        \"HF_TASK\": \"Image-Text-to-Text\",\n",
    "        \"OPTION_LIMIT_MM_PER_PROMPT\": \"image=2\", # Limit the number of images that can be sent per prompt\n",
    "       \"HUGGING_FACE_HUB_TOKEN\": HUGGING_FACE_HUB_TOKEN # HF Token for gated models\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f6fe9cf-a47c-4406-acbd-fd335ac08253",
   "metadata": {},
   "source": [
    "## Deploy Model to SageMaker Endpoint\n",
    "\n",
    "Now we'll deploy our model to a SageMaker endpoint for real-time inference. This is a significant step that:\n",
    "1. Provisions the specified compute resources (G6e instance)\n",
    "2. Deploys the model container\n",
    "3. Sets up the endpoint for API access\n",
    "\n",
    "### Deployment Configuration\n",
    "- **Instance Count**: 1 instance for single-node deployment\n",
    "- **Instance Type**: `ml.g6e.48xlarge` for high-performance inference\n",
    "\n",
    "> ⚠️ **Important**: \n",
    "> - Deployment can take up to 15 minutes\n",
    "> - Monitor the CloudWatch logs for progress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bafb2f42-4790-4c12-850e-b7482c05be69",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "from sagemaker.utils import name_from_base\n",
    "\n",
    "endpoint_name = name_from_base(base_name, short=True)\n",
    "\n",
    "gemma_3_model.deploy(\n",
    "    endpoint_name=endpoint_name,\n",
    "    initial_instance_count=1,\n",
    "    instance_type=gpu_instance_type\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc83950a-ff8d-4344-add3-6c88b48b8d36",
   "metadata": {},
   "source": [
    "### Use the code below to create a predictor from an existing endpoint and make inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff66831e-dc02-487f-a253-5caa915a98c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.serializers import JSONSerializer, IdentitySerializer\n",
    "from sagemaker.deserializers import JSONDeserializer\n",
    "from sagemaker.predictor import Predictor\n",
    "\n",
    "endpoint_name = \"<>\"# replace with your enpoint name \n",
    "\n",
    "gemma_3_predictor = Predictor(\n",
    "    endpoint_name=endpoint_name,\n",
    "    serializer=JSONSerializer(),\n",
    "    deserializer=JSONDeserializer()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dc088b5-0681-411b-9e1d-650736b34723",
   "metadata": {},
   "source": [
    "## Text only Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0da6a906-d894-4916-b5b0-87ed91fe8c2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "payload = {\n",
    "    \"messages\" : [\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [{\"type\": \"text\", \"text\": \"Write me a poem about Machine Learning.\"}]\n",
    "        }\n",
    "    ],\n",
    "    \"max_tokens\":300,\n",
    "    \"temperature\": 0.6,\n",
    "    \"top_p\": 0.9,\n",
    "}\n",
    "\n",
    "response = gemma_3_predictor.predict(payload)\n",
    "print(response['choices'][0]['message']['content'])\n",
    "\n",
    "# Print usage statistics\n",
    "print(\"=== Token Usage ===\")\n",
    "usage = response['usage']\n",
    "print(f\"Prompt Tokens: {usage['prompt_tokens']}\")\n",
    "print(f\"Completion Tokens: {usage['completion_tokens']}\")\n",
    "print(f\"Total Tokens: {usage['total_tokens']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3158db2a-96db-42c5-8c28-4942513a6950",
   "metadata": {},
   "source": [
    "## Multimodality\n",
    "\n",
    "Gemma 3 models are multimodal, handling text and image input and generating text output, with open weights for both pre-trained variants and instruction-tuned variants. Gemma 3 has a large, 128K context window, multilingual support in over 140 languages, and is available in more sizes than previous versions. Gemma 3 models are well-suited for a variety of text generation and image understanding tasks, including question answering, summarization, and reasoning. Their relatively small size makes it possible to deploy them in environments with limited resources such as laptops, desktops or your own cloud infrastructure, democratizing access to state of the art AI models and helping foster innovation for everyone."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f731037a-2649-4b40-b530-135d36ae706c",
   "metadata": {},
   "source": [
    "#### single image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36b8a8a0-fde2-4286-b1cf-30bf6b069800",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image as IPyImage\n",
    "IPyImage(url=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/p-blog/candy.JPG\", height=300, width= 300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d6ca443-c231-4935-a8d0-b101b2624835",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "payload = {\n",
    "  \"messages\": [\n",
    "    {\n",
    "      \"role\": \"system\",\n",
    "      \"content\": [{\"type\": \"text\", \"text\": \"You are a helpful assistant.\"}]\n",
    "    },\n",
    "    {\n",
    "      \"role\": \"user\",\n",
    "      \"content\": [\n",
    "        {\n",
    "          \"type\": \"image_url\", \n",
    "          \"image_url\": {\"url\": \"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/p-blog/candy.JPG\"},\n",
    "        },\n",
    "        {\"type\": \"text\", \"text\": \"What animal is on the candy?\"}\n",
    "      ]\n",
    "    }\n",
    "  ],\n",
    "    \"max_tokens\":300,\n",
    "    \"temperature\": 0.6,\n",
    "    \"top_p\": 0.9,\n",
    "}\n",
    "\n",
    "response = gemma_3_predictor.predict(payload)\n",
    "print(response['choices'][0]['message']['content'])\n",
    "\n",
    "# Print usage statistics\n",
    "print(\"=== Token Usage ===\")\n",
    "usage = response['usage']\n",
    "print(f\"Prompt Tokens: {usage['prompt_tokens']}\")\n",
    "print(f\"Completion Tokens: {usage['completion_tokens']}\")\n",
    "print(f\"Total Tokens: {usage['total_tokens']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f842f1e-1f59-496b-85ee-3b0a3bd85951",
   "metadata": {},
   "source": [
    "### Streaming responses\n",
    "You can also direclty stream response from your endpoint. To achieve this, we will use the invoke_endpoint_with_response_stream API."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20362629-77fe-4b57-b1ab-97259defc72b",
   "metadata": {},
   "source": [
    "You can **interleave images with text**. To do so, just cut off the input text where you want to insert an image, and insert it with an image block like the following."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54a9c62e-fb42-409a-b1f5-0c4084662fb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image as IPyImage\n",
    "IPyImage(url=\"https://huggingface.co/datasets/merve/vlm_test_images/resolve/main/IMG_3018.JPG\", height=300, width= 300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf3fb62f-16c3-4eef-9f6e-9f82ba78cc83",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image as IPyImage\n",
    "IPyImage(url=\"https://huggingface.co/datasets/merve/vlm_test_images/resolve/main/IMG_3015.jpg\", height=300, width= 300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b8d4f5f-fc32-468f-b4d0-3551c9327ac3",
   "metadata": {},
   "outputs": [],
   "source": [
    "body = {\n",
    "  \"messages\": [\n",
    "    {\n",
    "      \"role\": \"system\",\n",
    "      \"content\": [{\"type\": \"text\", \"text\": \"You are a helpful assistant.\"}]\n",
    "    },\n",
    "    {\n",
    "      \"role\": \"user\",\n",
    "      \"content\": [\n",
    "        {\"type\": \"text\", \"text\": \"I'm already using this supplement \"},\n",
    "        {\n",
    "          \"type\": \"image_url\", \n",
    "          \"image_url\": {\"url\": \"https://huggingface.co/datasets/merve/vlm_test_images/resolve/main/IMG_3018.JPG\"},\n",
    "        },\n",
    "        {\"type\": \"text\", \"text\": \"and I want to use this one too \"},\n",
    "        {\n",
    "          \"type\": \"image_url\", \n",
    "          \"image_url\": {\"url\": \"https://huggingface.co/datasets/merve/vlm_test_images/resolve/main/IMG_3015.jpg\"},\n",
    "        },\n",
    "        {\"type\": \"text\", \"text\": \" what are cautions?\"},\n",
    "      ]\n",
    "    }\n",
    "  ],\n",
    "     \"max_tokens\":1500,\n",
    "    \"temperature\": 0.6,\n",
    "    \"top_p\": 0.9,\n",
    "    \"stream\": True,   \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27e3b708-cf53-402d-8dff-d3b465775fd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import time\n",
    "\n",
    "# Create SageMaker Runtime client\n",
    "smr_client = boto3.client(\"sagemaker-runtime\")\n",
    "##Add your endpoint here \n",
    "endpoint_name = \"<>\"\n",
    "\n",
    "# Invoke the model\n",
    "response_stream = smr_client.invoke_endpoint_with_response_stream(\n",
    "    EndpointName=endpoint_name,\n",
    "    ContentType=\"application/json\",\n",
    "    Body=json.dumps(body)\n",
    ")\n",
    "\n",
    "first_token_received = False\n",
    "ttft = None\n",
    "token_count = 0\n",
    "start_time = time.time()\n",
    "\n",
    "print(\"Response:\", end=' ', flush=True)\n",
    "full_response = \"\"\n",
    "\n",
    "for event in response_stream['Body']:\n",
    "    if 'PayloadPart' in event:\n",
    "        chunk = event['PayloadPart']['Bytes'].decode()\n",
    "        \n",
    "        try:\n",
    "            # Handle SSE format (data: prefix)\n",
    "            if chunk.startswith('data: '):\n",
    "                data = json.loads(chunk[6:])  # Skip \"data: \" prefix\n",
    "            else:\n",
    "                data = json.loads(chunk)\n",
    "            \n",
    "            # Extract token based on OpenAI format\n",
    "            if 'choices' in data and len(data['choices']) > 0:\n",
    "                if 'delta' in data['choices'][0] and 'content' in data['choices'][0]['delta']:\n",
    "                    token_count += 1\n",
    "                    token_text = data['choices'][0]['delta']['content']\n",
    "                                    # Record time to first token\n",
    "                    if not first_token_received:\n",
    "                        ttft = time.time() - start_time\n",
    "                        first_token_received = True\n",
    "                    full_response += token_text\n",
    "                    print(token_text, end='', flush=True)\n",
    "        \n",
    "        except json.JSONDecodeError:\n",
    "            continue\n",
    "            \n",
    "# Print metrics after completion\n",
    "end_time = time.time()\n",
    "total_latency = end_time - start_time\n",
    "\n",
    "print(\"\\n\\nMetrics:\")\n",
    "print(f\"Time to First Token (TTFT): {ttft:.2f} seconds\" if ttft else \"TTFT: N/A\")\n",
    "print(f\"Total Tokens Generated: {token_count}\")\n",
    "print(f\"Total Latency: {total_latency:.2f} seconds\")\n",
    "if token_count > 0 and total_latency > 0:\n",
    "    print(f\"Tokens per second: {token_count/total_latency:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b9783d3-a8e4-4c86-81f8-054e2175ce5a",
   "metadata": {},
   "source": [
    "# Clean up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7984819c-e3ec-47d9-92a8-d91fa4998b55",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Clean up\n",
    "gemma_3_predictor.delete_model()\n",
    "gemma_3_predictor.delete_endpoint(delete_endpoint_config=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
