{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "993dcf59-dd85-4a31-a8cc-02c5d4be5dfe",
   "metadata": {},
   "source": [
    "# Deploy Llama 4 Scout on SageMaker AI with Hugging Face Text Generation Inference.\n",
    "\n",
    "The Llama 4 collection of models are multimodal AI models developed by Meta. These models leverage a mixture-of-experts architecture for industry leading text and image understanding.\n",
    "\n",
    "Llama 4 Scout is a 17 billion parameter model with 16 experts released on April 5th 2025. For more information, please refer to the model card [here](https://huggingface.co/meta-llama/Llama-4-Scout-17B-16E-Instruct)\n",
    "\n",
    "We can deploy Llama 4 Scout on a SageMaker AI Endpoint using Text Generation Inference (TGI). TGI is a toolkit for deploying and serving large language models developed by Hugging Face available for use in Amazon SageMaker AI. For more information on TGI, refer to [here](https://huggingface.co/docs/text-generation-inference/en/index).\n",
    "\n",
    "---\n",
    "\n",
    "To get started, we should update our SageMaker Python SDK and configure our role and session information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c01a4b2-b0d3-420c-8335-4efd984bb5dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install sagemaker --upgrade --quiet --no-warn-conflicts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d192f870-74ee-4470-baf5-93729291a6e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import sagemaker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9183bf15-3f3f-4d70-91da-fe269ff421b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "role = sagemaker.get_execution_role()  # execution role for the endpoint\n",
    "sess = sagemaker.session.Session()  # sagemaker session for interacting with different AWS APIs\n",
    "bucket = sess.default_bucket()  # bucket to house artifacts\n",
    "region = sess._region_name  # region name of the current SageMaker Studio environment\n",
    "\n",
    "print(f\"sagemaker role arn: {role}\")\n",
    "print(f\"sagemaker bucket: {sess.default_bucket()}\")\n",
    "print(f\"sagemaker session region: {sess.boto_region_name}\")\n",
    "print(f\"sagemaker version: {sagemaker.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ff7ffcf-f1fb-46d7-adad-00d18b561f09",
   "metadata": {},
   "source": [
    "We set the ECR image URI for the Hugging Face Text Generation Inference container. Version 3.2.2 is updated to support the Llama 4 collection of models so the container must be at least version 3.2.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f91002f-04c1-450f-a414-ea262870a1cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.huggingface import get_huggingface_llm_image_uri\n",
    "\n",
    "# OVERRIDE:\n",
    "llm_image = f\"763104351884.dkr.ecr.{sess.boto_region_name}.amazonaws.com/huggingface-pytorch-tgi-inference:2.6.0-tgi3.2.3-gpu-py311-cu124-ubuntu22.04-v2.0\"\n",
    "\n",
    "print(f\"llm image uri: {llm_image}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1634e71f-a190-408f-bcb9-a314cd057e30",
   "metadata": {},
   "source": [
    "## Deploying an endpoint\n",
    "\n",
    "We can configure our endpoint with the SageMaker Python SDK to deploy our model. As Llama 4 models are gated models, please ensure that you have been granted access and provide a valid token for use.\n",
    "\n",
    "We also need to configure our instance type and environment variables. Llama 4 Scout is a Mixture-of-Experts (MOE) model with 16 billion active parameters out of 109 billion in total. As all these parameters are stored in memory, we need a large amount of GPU memory available. Here we select the `ml.p4d.24xlarge` which has 320GB of GPU memory across 8 A100 GPUs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d0c37f0-6a2e-4a0c-8630-70429dacaabf",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = sagemaker.utils.name_from_base(\"llama4-tgi\")\n",
    "endpoint_name = model_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc64cd9b-b418-49f6-aac5-8cda8740736d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.huggingface import HuggingFaceModel\n",
    "\n",
    "# sagemaker config\n",
    "instance_type = \"ml.p4d.24xlarge\"\n",
    "number_of_gpu = 8\n",
    "health_check_timeout = 1800\n",
    "\n",
    "# TGI config\n",
    "config = {\n",
    "    \"HF_MODEL_ID\": \"meta-llama/Llama-4-Scout-17B-16E-Instruct\",\n",
    "    \"HUGGING_FACE_HUB_TOKEN\": \"<REPLACE WITH YOUR TOKEN>\",\n",
    "    'SM_NUM_GPUS': json.dumps(number_of_gpu), # Number of GPU used per replica\n",
    "    'MAX_INPUT_LENGTH': json.dumps(4096),  # Max length of input text\n",
    "    'MAX_TOTAL_TOKENS': json.dumps(8192),  # Max length of the generation (including input text)\n",
    "}\n",
    "\n",
    "assert config['HUGGING_FACE_HUB_TOKEN'] != '<REPLACE WITH YOUR TOKEN>', \"You have to provide a token.\"\n",
    "\n",
    "# create HuggingFaceModel\n",
    "llm_model = HuggingFaceModel(\n",
    "    role = role,\n",
    "    image_uri = llm_image,\n",
    "    env = config,\n",
    "    name = model_name\n",
    ")\n",
    "\n",
    "# Deploy model to an endpoint\n",
    "# https://sagemaker.readthedocs.io/en/stable/api/inference/model.html#sagemaker.model.Model.deploy\n",
    "llm = llm_model.deploy(\n",
    "    initial_instance_count = 1,\n",
    "    instance_type = instance_type,\n",
    "    container_startup_health_check_timeout = health_check_timeout,\n",
    "    endpoint_name = endpoint_name,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa7b64dd-7142-4c8d-a6d3-3e045032e65d",
   "metadata": {},
   "source": [
    "## Inference\n",
    "Once our model is deployed we can invoke it with the `predict` method as below for synchronous inference and see the response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72c6a984-289d-44f2-9a42-5ecbf35744ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = sagemaker.Predictor(\n",
    "    endpoint_name = endpoint_name,\n",
    "    sagemaker_session = sess,\n",
    "    serializer = sagemaker.serializers.JSONSerializer(),\n",
    "    deserializer = sagemaker.deserializers.JSONDeserializer(),\n",
    ")\n",
    "\n",
    "prompt = \"What is Amazon SageMaker?\"\n",
    "\n",
    "res = llm.predict({\"inputs\": prompt, \"parameters\": {\"temperature\": 0.9, \"max_tokens\": 1024}})\n",
    "print(res[0][\"generated_text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d503cf3-8d53-46c9-a20a-06de592f5c9d",
   "metadata": {},
   "source": [
    "The Llama 4 models are multi-modal meaning they can work with image and text inputs. We can use this for image understanding usecases.\n",
    "\n",
    "To start, we can define some helper functions for prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cfb06ba-706a-40df-99dc-8b5fea230a2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "from PIL import Image\n",
    "import requests\n",
    "from io import BytesIO\n",
    "\n",
    "runtime = boto3.client('sagemaker-runtime')\n",
    "\n",
    "def get_image_urls(payload):\n",
    "    image_urls = []\n",
    "    payload = json.loads(payload)\n",
    "    for msg in payload[\"messages\"]:\n",
    "        if type(msg[\"content\"]) == list:\n",
    "            for ms in msg[\"content\"]:\n",
    "                typ = ms[\"type\"]\n",
    "                if typ == \"image_url\":\n",
    "                    image_url = ms[\"image_url\"][\"url\"]\n",
    "                    image_urls.append(image_url)\n",
    "    return image_urls\n",
    "\n",
    "def display_images(image_paths):\n",
    "    \"\"\"\n",
    "    Displays multiple images side by side using PIL.\n",
    "\n",
    "    Args:\n",
    "        image_paths: A list of file paths to the images.\n",
    "    \"\"\"\n",
    "    responses = [ requests.get(url) for url in image_paths]\n",
    "    images = [Image.open(BytesIO(response.content)) for response in responses]\n",
    "    widths, heights = zip(*(i.size for i in images))\n",
    "\n",
    "    total_width = sum(widths)\n",
    "    max_height = max(heights)\n",
    "\n",
    "    new_image = Image.new('RGB', (total_width, max_height))\n",
    "\n",
    "    x_offset = 0\n",
    "    for image in images:\n",
    "        new_image.paste(image, (x_offset, 0))\n",
    "        x_offset += image.size[0]\n",
    "\n",
    "    new_image.show()\n",
    "\n",
    "def predict(payload, endpoint_name, imgs=False):\n",
    "    response = runtime.invoke_endpoint(EndpointName=endpoint_name,\n",
    "                                       ContentType='application/json',\n",
    "                                       Body=payload)\n",
    "    if imgs:\n",
    "        image_urls = get_image_urls(payload)\n",
    "        display_images(image_urls)\n",
    "    result = json.loads(response['Body'].read().decode())\n",
    "    return result[\"choices\"][0][\"message\"][\"content\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a65c4a6-0fb1-43f5-829a-acf9df998daf",
   "metadata": {},
   "source": [
    "We use the Messages API format to invoke Llama 4 Scout with a URL of an image. On invocation, the model is able to access the URL and provide an output based on the image.\n",
    "\n",
    "Below we use it to describe an image of a rabbit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b42b7089-2d42-4425-880c-d9dd1e10791f",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {\n",
    "        \"messages\": [\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": \"You are a helpful assistant\",\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": [\n",
    "                    {\n",
    "                        \"type\": \"text\",\n",
    "                        \"text\": \"Describe this image in detail please.\",\n",
    "                    },\n",
    "                    {\n",
    "                        \"type\": \"image_url\",\n",
    "                        \"image_url\": {\n",
    "                            \"url\": \"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/rabbit.png\",\n",
    "                        },\n",
    "                    },\n",
    "                ],\n",
    "            },\n",
    "        ],\n",
    "        \"temperature\": 0.6,\n",
    "        \"top_p\": 0.9,\n",
    "        \"max_tokens\": 512\n",
    "    }\n",
    "payload = json.dumps(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84798b54-908d-4226-9a32-5835e9c4b3fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "predict(payload, endpoint_name, imgs=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35dc3f81-636b-41f7-ae60-d568c9a5bceb",
   "metadata": {},
   "source": [
    "We can also encode images in base64 for inference as below. Note that the image bytes must be prefixed with `data:image/png;base64,`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1604af83-77b4-4bcd-a37f-7421b9701a04",
   "metadata": {},
   "outputs": [],
   "source": [
    "!curl -o rabbit.png https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/rabbit.png"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cac1f1b6-851e-4312-b593-936606370394",
   "metadata": {},
   "outputs": [],
   "source": [
    "import base64\n",
    "\n",
    "def image_to_base64_data_uri(file_path):\n",
    "    with open(file_path, \"rb\") as img_file:\n",
    "        base64_data = base64.b64encode(img_file.read()).decode('utf-8')\n",
    "        return f\"{base64_data}\"\n",
    "\n",
    "# Replace 'file_path.png' with the actual path to your PNG file\n",
    "file_path = 'rabbit.png'\n",
    "data_uri = image_to_base64_data_uri(file_path)\n",
    "\n",
    "data = {\n",
    "        \"messages\": [\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": \"You are a helpful assistant\",\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": [\n",
    "                    {\n",
    "                        \"type\": \"text\",\n",
    "                        \"text\": \"Describe this image in detail please.\",\n",
    "                    },\n",
    "                    {\n",
    "                        \"type\": \"image_url\",\n",
    "                        \"image_url\": {\n",
    "                            \"url\": f\"data:image/png;base64,{data_uri}\",\n",
    "                        },\n",
    "                    },\n",
    "                ],\n",
    "            },\n",
    "        ],\n",
    "        \"temperature\": 0.6,\n",
    "        \"top_p\": 0.9,\n",
    "        \"max_tokens\": 512\n",
    "    }\n",
    "payload = json.dumps(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80ff4072-677f-405d-9e91-106d29a114b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "predict(payload, endpoint_name, imgs=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d597146e-edc7-43c8-9021-b57d727636f1",
   "metadata": {},
   "source": [
    "## Cleanup\n",
    "\n",
    "Once we are done, we delete our endpoint and model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2fa6949-2916-4fee-85e7-a9c02d94df9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "sess.delete_endpoint(endpoint_name)\n",
    "sess.delete_endpoint_config(endpoint_name)\n",
    "sess.delete_model(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a015a721-a633-4373-afaa-c27b4a8cdfde",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
