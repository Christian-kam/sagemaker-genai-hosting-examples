{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9fd2fa6e",
   "metadata": {},
   "source": [
    "# How to deploy Black Forest Labs's FLUX.1-schnell for inference on Amazon SageMakerAI"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e361b0e0-c72e-441f-860e-353624db58b8",
   "metadata": {},
   "source": [
    "In this notebook, you will learn how to deploy **Black Forest Labs's FLUX.1-schnell** model (HuggingFace model ID: [black-forest-labs/FLUX.1-schnell](https://huggingface.co/black-forest-labs/FLUX.1-schnell)) using Amazon SageMaker AI. The inference image will be the SageMaker-managed [LMI (Large Model Inference) v15](https://docs.aws.amazon.com/sagemaker/latest/dg/large-model-inference-container-docs.html) Docker image. LMI images features a [DJL serving](https://github.com/deepjavalibrary/djl-serving) stack powered by the [Deep Java Library](https://djl.ai/). \n",
    "\n",
    "FLUX.1 [schnell] is a 12 billion parameter rectified flow transformer capable of generating images from text descriptions. For more information, please read our [blog post](https://blackforestlabs.ai/announcing-black-forest-labs/).\n",
    "\n",
    "### Key Features\n",
    "\n",
    "- Cutting-edge output quality and competitive prompt following, matching the performance of closed source alternatives.\n",
    "- Trained using latent adversarial diffusion distillation, FLUX.1 [schnell] can generate high-quality images in only 1 to 4 steps.\n",
    "- Released under the apache-2.0 licence, the model can be used for personal, scientific, and commercial purposes.\n",
    "\n",
    "### Usage\n",
    "\n",
    "We provide a reference implementation of FLUX.1 [schnell], as well as sampling code, in a dedicated github repository. Developers and creatives looking to build on top of FLUX.1 [schnell] are encouraged to use this as a starting point.\n",
    "\n",
    "### Out-of-Scope Use \n",
    "The model and its derivatives may not be used\n",
    "\n",
    "- In any way that violates any applicable national, federal, state, local or international law or regulation.\n",
    "- For the purpose of exploiting, harming or attempting to exploit or harm minors in any way; including but not limited to the solicitation, creation, acquisition, or dissemination of child exploitative content.\n",
    "- To generate or disseminate verifiably false information and/or content with the purpose of harming others.\n",
    "- To generate or disseminate personal identifiable information that can be used to harm an individual.\n",
    "- To harass, abuse, threaten, stalk, or bully individuals or groups of individuals.\n",
    "- To create non-consensual nudity or illegal pornographic content.\n",
    "- For fully automated decision making that adversely impacts an individual's legal rights or otherwise creates or modifies a binding, enforceable obligation.\n",
    "- Generating or facilitating large-scale disinformation campaigns.\n",
    "\n",
    "\n",
    "### License agreement\n",
    "* This model is gated on HuggingFace, please refer to the original [model card](https://huggingface.co/black-forest-labs/FLUX.1-schnell) for license.\n",
    "* Tailored for local development and personal use. [FLUX.1 schnell](https://huggingface.co/black-forest-labs/FLUX.1-schnell) is openly available under an Apache2.0 license\n",
    "* This notebook is a sample notebook and not intended for production use."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "318937da",
   "metadata": {},
   "source": [
    "## Payload format to invoke the model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5ad4cc4",
   "metadata": {},
   "source": [
    "Below you can find an examples for a request. \n",
    "\n",
    "```python\n",
    "{\n",
    "    \"prompt\": \"A cat holding a sign that says hello world\",\n",
    "    \"guidance_scale\": 0.0,\n",
    "    \"num_inference_steps\": 4,\n",
    "    \"max_sequence_length\": 256,\n",
    "    \"seed\": 42\n",
    "}\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52ac63d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -Uq sagemaker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16122817-b2d2-4097-9be7-48b8777fd04e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "import boto3\n",
    "import logging\n",
    "import time\n",
    "from sagemaker.session import Session\n",
    "from sagemaker.s3 import S3Uploader\n",
    "\n",
    "print(sagemaker.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa458b5a-f099-4f4c-8fc0-185d305a6e0e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    boto_region = boto3.Session().region_name\n",
    "    sm_session = sagemaker.session.Session(boto_session=boto3.Session(region_name=boto_region))\n",
    "    role = sagemaker.get_execution_role()\n",
    "    sagemaker_default_bucket = sm_session.default_bucket()\n",
    "    \n",
    "except ValueError:\n",
    "    iam = boto3.client('iam')\n",
    "    role = iam.get_role(RoleName='sagemaker_execution_role')['Role']['Arn']\n",
    "\n",
    "print(f\"sagemaker role arn: {role}\")\n",
    "print(f\"sagemaker default bucket: {sagemaker_default_bucket}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33bd1740-9559-4dad-86d9-37ac97deb616",
   "metadata": {},
   "outputs": [],
   "source": [
    "HF_MODEL_ID = \"black-forest-labs/FLUX.1-schnell\"\n",
    "\n",
    "base_name = HF_MODEL_ID.split('/')[-1].replace('.', '-').lower()\n",
    "model_lineage = HF_MODEL_ID.split(\"/\")[0]\n",
    "base_name"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c02c56c9-1fa0-470f-8b51-aecd0509028c",
   "metadata": {},
   "source": [
    "## Download the model from Hugging Face and upload the model artifacts on Amazon S3\n",
    "If you are deploying a model hosted on the HuggingFace Hub, you must specify the `option.model_id=<hf_hub_model_id>` configuration. When using a model directly from the hub, we recommend you also specify the model revision (commit hash or branch) via `option.revision=<commit hash/branch>`. \n",
    "\n",
    "Since model artifacts are downloaded at runtime from the Hub, using a specific revision ensures you are using a model compatible with package versions in the runtime environment. Open Source model artifacts on the hub are subject to change at any time. These changes may cause issues when instantiating the model (updated model artifacts may require a newer version of a dependency than what is bundled in the container). If a model provides custom model (modeling.py) and/or custom tokenizer (tokenizer.py) files, you need to specify option.trust_remote_code=true to load and use the model.\n",
    "\n",
    "In this example, we will demonstrate how to download your copy of the model from huggingface and upload it to an s3 location in your AWS account, then deploy the model with the downloaded model artifacts to an endpoint.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f62ecc10-fbed-4cd8-b239-c88fea1f582b",
   "metadata": {},
   "source": [
    "**Best Practices**:\n",
    ">\n",
    "> **Store Models in Your Own S3 Bucket**\n",
    "For production use-cases, always download and store model files in your own S3 bucket to ensure validated artifacts. This provides verified provenance, improved access control, consistent availability, protection against upstream changes, and compliance with organizational security protocols.\n",
    ">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93710574-0e01-4e62-8b15-92f604a9900b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import snapshot_download\n",
    "from pathlib import Path\n",
    "\n",
    "model_dir = Path('model-files')\n",
    "model_dir.mkdir(exist_ok=True)\n",
    "\n",
    "snapshot_download(HF_MODEL_ID, local_dir=model_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10f2fb57-94ab-4120-841f-78790b54bbc0",
   "metadata": {},
   "source": [
    "### Upload model files to S3 in uncompress format for SageMaker AI\n",
    "SageMaker AI allows us to provide [uncompressed](https://docs.aws.amazon.com/sagemaker/latest/dg/large-model-inference-uncompressed.html) files. Thus, we directly upload the folder that contains model files to s3\n",
    "> **Note**: The default SageMaker bucket follows the naming pattern: `sagemaker-{region}-{account-id}`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b089df36-efa5-457a-9750-04a0641b15d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# upload uncompress model files to s3\n",
    "model_artifact_uri = S3Uploader.upload(\n",
    "    local_path=\"./model-files\",\n",
    "    desired_s3_uri=f\"s3://{sagemaker_default_bucket}/lmi/{base_name}\"\n",
    ")\n",
    "print(f\"Model files are uploaded to --- >: {model_artifact_uri}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90f7bee3-6260-40b4-a9ca-652d1fd96963",
   "metadata": {},
   "source": [
    "### Configure Model Serving Properties and model.py that will be used to load the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "218d73c3-a3d5-4807-921c-9c1ba48421e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the directory that will contain the configuration files\n",
    "from pathlib import Path\n",
    "\n",
    "model_dir = Path('config')\n",
    "model_dir.mkdir(exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b06d9f5-ac8e-4964-bb7b-2c48a4fc2a2d",
   "metadata": {},
   "source": [
    "**Best Practices**:\n",
    ">\n",
    ">**Separate Configuration from Model Artifacts**\n",
    "> The LMI container supports separating configuration files from model artifacts. While you can store serving.properties with your model files, placing configurations in a distinct S3 location allows for better management of all your configurations files.\n",
    ">\n",
    "> **Note**: When your model and configuration files are in different S3 locations, set `option.model_id=<s3_model_uri>` in your serving.properties file, where `s3_model_uri` is the S3 object prefix containing your model artifacts. SageMaker AI will automatically download the model files by looking at the S3URI in model_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "509b96c2-ac17-48ad-9849-cf81182e67cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile ./config/model.py\n",
    "import torch\n",
    "from diffusers import FluxPipeline\n",
    "import base64\n",
    "from io import BytesIO\n",
    "from djl_python import Input, Output\n",
    "import os\n",
    "\n",
    "class FluxModelHandler(object):\n",
    "    def __init__(self):\n",
    "        self.pipe = None\n",
    "        self.device = None\n",
    "        # Initialize the model immediately when the class is instantiated\n",
    "        self._load_model()\n",
    "\n",
    "    def _load_model(self):\n",
    "        \"\"\"Load the model once during container startup\"\"\"\n",
    "        self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "        print(f\"Using device: {self.device}\")\n",
    "    \n",
    "        # Use the DJL download location\n",
    "        djl_base_path = \"/tmp/.djl.ai/download\"\n",
    "        model_path = None\n",
    "    \n",
    "        if os.path.exists(djl_base_path):\n",
    "            # Find the first directory that contains model files\n",
    "            for item in os.listdir(djl_base_path):\n",
    "                potential_path = os.path.join(djl_base_path, item)\n",
    "                if os.path.isdir(potential_path) and os.path.exists(os.path.join(potential_path, \"model_index.json\")):\n",
    "                    model_path = potential_path\n",
    "                    break\n",
    "    \n",
    "        if model_path:\n",
    "            print(f\"Loading model from DJL download location: {model_path}\")\n",
    "            self.pipe = FluxPipeline.from_pretrained(model_path, torch_dtype=torch.bfloat16)\n",
    "        else:\n",
    "            raise FileNotFoundError(\"Model not found, please make sure the model files are downloaded from s3\")\n",
    "    \n",
    "        if self.device == \"cuda\":\n",
    "            self.pipe.enable_model_cpu_offload()\n",
    "        \n",
    "        print(\"Model loaded successfully and ready for inference!\")\n",
    "\n",
    "    def handle(self, inputs: Input) -> Output:\n",
    "        # Model is already loaded, no need to check initialization\n",
    "        try:\n",
    "            input_data = inputs.get_as_json()\n",
    "            prompt = input_data.get(\"prompt\", \"A cat holding a sign that says hello world\")\n",
    "            guidance_scale = float(input_data.get(\"guidance_scale\", 0.0))\n",
    "            num_inference_steps = int(input_data.get(\"num_inference_steps\", 4))\n",
    "            max_sequence_length = int(input_data.get(\"max_sequence_length\", 256))\n",
    "            seed = int(input_data.get(\"seed\", 0))\n",
    "\n",
    "            generator = torch.Generator(self.device).manual_seed(seed)\n",
    "\n",
    "            image = self.pipe(\n",
    "                prompt,\n",
    "                guidance_scale=guidance_scale,\n",
    "                num_inference_steps=num_inference_steps,\n",
    "                max_sequence_length=max_sequence_length,\n",
    "                generator=generator\n",
    "            ).images[0]\n",
    "\n",
    "            # Convert to base64\n",
    "            buffered = BytesIO()\n",
    "            image.save(buffered, format=\"PNG\")\n",
    "            img_str = base64.b64encode(buffered.getvalue()).decode()\n",
    "\n",
    "            output = Output()\n",
    "            output.add_as_json({\"generated_image\": img_str})\n",
    "            return output\n",
    "\n",
    "        except Exception as e:\n",
    "            error_output = Output()\n",
    "            error_output.add_as_json({\"error\": str(e)})\n",
    "            return error_output\n",
    "\n",
    "\n",
    "# Create the service instance once when the module is imported\n",
    "_service = FluxModelHandler()\n",
    "\n",
    "\n",
    "def handle(inputs: Input) -> Output:\n",
    "    return _service.handle(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e7951ad-c9c7-4d17-bcf8-f72a77aeee5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = f\"\"\"engine=Python\n",
    "option.tensor_parallel_degree=max\n",
    "option.model_loading_timeout=1500\n",
    "option.async_mode=false\n",
    "option.entryPoint=model.py\n",
    "option.model_id={model_artifact_uri}\n",
    "option.trust_remote_code=false\n",
    "option.dtype=bfloat16\n",
    "fail_fast=true\n",
    "\"\"\"\n",
    "with open(\"config/serving.properties\", \"w\") as f:\n",
    "    f.write(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90311de0-778a-416f-87a6-f3881c050e32",
   "metadata": {},
   "source": [
    "#### Optional configuration files\n",
    "\n",
    "(Optional) You can also specify a `requirements.txt` to install additional libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f60a3ef0-4214-4585-b0b4-72b169a2fe3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile config/requirements.txt\n",
    "peft==0.15.1\n",
    "diffusers==0.34.0\n",
    "transformers==4.51.3\n",
    "accelerate==1.0.1\n",
    "pillow==11.2.1\n",
    "torch==2.6.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e9cecfe-7795-44b7-b50b-a4ee930ab27b",
   "metadata": {},
   "source": [
    "### Upload config files to S3\n",
    "Here we will upload our config files to a different path to keep model files and config separate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f1702c9-57d8-4935-96f8-6123489dc9f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# upload the code and config to s3\n",
    "s3_config_prefix = f\"large-model-lmi/code-files-{model_lineage}-{base_name}\"\n",
    "\n",
    "configuration_files = S3Uploader.upload(\n",
    "    local_path=\"config\",\n",
    "    desired_s3_uri=f\"s3://{sagemaker_default_bucket}/{s3_config_prefix}\"\n",
    ")\n",
    "\n",
    "print(f\"Configuration files are uploaded to: {configuration_files}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e10ecbd2-5454-458c-ad16-855a1c6db6d1",
   "metadata": {},
   "source": [
    "## Configure Model Container and Instance\n",
    "\n",
    "For deploying Flux-1-Schnell, we'll use:\n",
    "- **LMI (Deep Java Library) Inference Container**: A container optimized for large language model inference\n",
    "- **[G6e Instance](https://aws.amazon.com/ec2/instance-types/g6e/)**: AWS's GPU instance type powered by NVIDIA L40S Tensor Core GPUs \n",
    "\n",
    "Key configurations:\n",
    "- The container URI points to the DJL inference container in ECR (Elastic Container Registry)\n",
    "- We use `ml.g6e.4xlarge` instance\n",
    "> **Note**: The region in the container URI should match your AWS region."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37ba3c28-abb1-4863-87a3-eff7a83c6817",
   "metadata": {},
   "outputs": [],
   "source": [
    "CONTAINER_VERSION = '0.33.0-lmi15.0.0-cu128'\n",
    "image_uri = \"763104351884.dkr.ecr.{}.amazonaws.com/djl-inference:{}\".format(sm_session.boto_session.region_name, CONTAINER_VERSION)\n",
    "print(image_uri)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0827b586-872f-40ed-9c53-d60d06a07f03",
   "metadata": {},
   "outputs": [],
   "source": [
    "gpu_instance_type = \"ml.g6e.4xlarge\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc505e58-7bbe-4d7e-890d-fbb1c9506199",
   "metadata": {},
   "source": [
    "## Create SageMaker Model\n",
    "\n",
    "Now we'll create a SageMaker Model object that combines our:\n",
    "- Container image (LMI)\n",
    "- code artifacts (configuration files)\n",
    "- IAM role (for permissions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fda0e0b8-4db7-46f2-aba4-bad48395c1fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the S3 URI for your uncompressed config files\n",
    "config_data = {\n",
    "    \"S3DataSource\": {\n",
    "        \"S3Uri\": f\"{configuration_files}/\",\n",
    "        \"S3DataType\": \"S3Prefix\",\n",
    "        \"CompressionType\": \"None\"\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8f0073b-7af5-49e9-8d50-6f1957467ada",
   "metadata": {},
   "source": [
    "> **Note**: Here S3 URI points to the configuration files S3 location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e5949a8-2b60-4d28-9ef6-09749d3fc25d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.utils import name_from_base\n",
    "from sagemaker.model import Model\n",
    "\n",
    "model_name = name_from_base(base_name, short=True)\n",
    "\n",
    "# Create model\n",
    "black_forest_model = Model(\n",
    "    name=model_name,\n",
    "    image_uri=image_uri,\n",
    "    model_data=config_data,  # Path to your model files\n",
    "    role=role,\n",
    "    env={\n",
    "        'HF_TASK':'text-to-image',\n",
    "    },\n",
    "    sagemaker_session=sm_session\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "031157ca-4bab-488b-8a8a-77a3809e4a06",
   "metadata": {},
   "source": [
    "## Deploy Model to SageMaker Endpoint\n",
    "\n",
    "Now we'll deploy our model to a SageMaker endpoint for real-time inference. \n",
    "> ⚠️ **Important**: \n",
    "> - Deployment can take up to 15 minutes\n",
    "> - Monitor the CloudWatch logs for progress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfba710c-fd77-41dd-9f15-b0fc17fc3d39",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "from sagemaker.serializers import JSONSerializer, IdentitySerializer\n",
    "from sagemaker.deserializers import JSONDeserializer\n",
    "from sagemaker.utils import name_from_base\n",
    "\n",
    "\n",
    "endpoint_name = name_from_base(base_name, short=True)\n",
    "instance_type = gpu_instance_type\n",
    "\n",
    "black_forest_model.deploy(\n",
    "    endpoint_name=endpoint_name,\n",
    "    initial_instance_count=1,\n",
    "    instance_type=instance_type,\n",
    "    serializer=JSONSerializer(),\n",
    "    deserializer=JSONDeserializer()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13220385-066f-49c3-b43f-922878d982bf",
   "metadata": {},
   "source": [
    "### Create a predictor from our existing endpoint and make inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47dcaee3-4d96-4668-8b31-0439767a94fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "from sagemaker.serializers import JSONSerializer, IdentitySerializer\n",
    "from sagemaker.deserializers import JSONDeserializer\n",
    "from sagemaker.predictor import Predictor\n",
    "\n",
    "endpoint_name = \"flux-1-schnell-250630-1656\"\n",
    "\n",
    "predictor = Predictor(\n",
    "    endpoint_name=endpoint_name,\n",
    "    serializer=JSONSerializer(),\n",
    "    deserializer=JSONDeserializer(),\n",
    "    sagemaker_session=sm_session\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "012d7ab5-3b72-4407-9174-2bdee70988b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Make a prediction\n",
    "payload = {\n",
    "    \"prompt\": \"whimsical and ethereal soft-shaded story illustration: A woman in a large hat stands at the ship's railing looking out across the ocean\",\n",
    "    \"guidance_scale\": 0.0,\n",
    "    \"num_inference_steps\": 4,\n",
    "    \"max_sequence_length\": 256,\n",
    "    \"seed\": 42\n",
    "}\n",
    "\n",
    "response = predictor.predict(payload)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9cf505e-c8d0-4db5-bb76-4ae5659c6be7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you want to convert the base64 image back to a PIL Image:\n",
    "import base64\n",
    "from PIL import Image\n",
    "import io\n",
    "\n",
    "# Extract the base64-encoded image data from the response\n",
    "base64_image = response['generated_image']\n",
    "\n",
    "# Decode the base64 string to bytes\n",
    "image_bytes = base64.b64decode(base64_image)\n",
    "\n",
    "# Create a PIL Image from the bytes\n",
    "generated_image = Image.open(io.BytesIO(image_bytes))\n",
    "\n",
    "# Display the image (this will open in your default image viewer)\n",
    "generated_image.show()\n",
    "\n",
    "# Save the image to a file\n",
    "generated_image.save('generated_image.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3ef3aa3-fdee-4e7a-832a-87dd89fe51cc",
   "metadata": {},
   "source": [
    "### Inference using boto3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fee7241e-bee5-48cd-84ba-04842bbbb88a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import json\n",
    "import base64\n",
    "from PIL import Image\n",
    "import io\n",
    "\n",
    "endpoint_name =\"flux-1-schnell-250630-1656\"\n",
    "prompt = \"A cat holding a sign that says hello world\"\n",
    "\n",
    "def test_endpoint(endpoint_name, prompt):\n",
    "    runtime = boto3.client('sagemaker-runtime', region_name=\"eu-west-3\")\n",
    "    \n",
    "    # Prepare the payload\n",
    "    payload = {\n",
    "        \"prompt\": prompt,\n",
    "        \"guidance_scale\": 0.0,\n",
    "        \"num_inference_steps\": 4,\n",
    "        \"max_sequence_length\": 256,\n",
    "        \"seed\": 42\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        response = runtime.invoke_endpoint(\n",
    "            EndpointName=endpoint_name,\n",
    "            ContentType='application/json',\n",
    "            Body=json.dumps(payload)\n",
    "        )\n",
    "        \n",
    "        result = json.loads(response['Body'].read())\n",
    "        print(\"Success!\")\n",
    "        return result\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {str(e)}\")\n",
    "        raise\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57c6fc2f-93ab-4d04-b10d-84ee9771fe63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the endpoint\n",
    "response = test_endpoint(endpoint_name, prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc2a4868-1b79-48ef-a2d3-d47410c3d06f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import base64\n",
    "from PIL import Image\n",
    "from IPython.display import display\n",
    "import io\n",
    "\n",
    "# Extract the base64-encoded image data from the response\n",
    "base64_image = response['generated_image']\n",
    "\n",
    "# Decode the base64 string to bytes\n",
    "image_bytes = base64.b64decode(base64_image)\n",
    "\n",
    "# Create a PIL Image from the bytes\n",
    "generated_image = Image.open(io.BytesIO(image_bytes))\n",
    "\n",
    "# After creating the PIL image\n",
    "max_size = (800, 800) \n",
    "generated_image.thumbnail(max_size, Image.Resampling.LANCZOS)\n",
    "display(generated_image)\n",
    "\n",
    "# Save the resized image to a file\n",
    "generated_image.save('generated_image.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11576c91",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# delete endpoint\n",
    "predictor.delete_model()\n",
    "predictor.delete_endpoint(delete_endpoint_config=True)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "c281c456f1b8161c8906f4af2c08ed2c40c50136979eaae69688b01f70e9f4a9"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
