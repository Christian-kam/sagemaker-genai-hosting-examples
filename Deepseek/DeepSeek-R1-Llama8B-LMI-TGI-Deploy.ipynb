{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a8fd722b-7c0b-4813-ae7f-963215645fb9",
   "metadata": {},
   "source": [
    "# ðŸš€ Deploy DeepSeek R1 Large Language Model from HuggingFace Hub on Amazon SageMaker"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a414500-d1e8-40d8-ac2c-861f385014fc",
   "metadata": {},
   "source": [
    "## Introduction: [DeepSeek R1](https://huggingface.co/deepseek-ai/DeepSeek-R1-Distill-Llama-8B)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d86f7a7-3d25-44f4-83c2-3b62c0c5763e",
   "metadata": {},
   "source": [
    "DeepSeek-R1 is an open-source reasoning model developed by [DeepSeek](https://www.deepseek.com/). It is designed to handle tasks requiring logical inference, mathematical problem-solving, and real-time decision-making. Notably, DeepSeek-R1 achieves performance comparable to leading Foundation Models across various benchmarks, including math, code, and reasoning tasks. \n",
    "\n",
    "The DeepSeek-R1 series includes several variants, each with distinct training methodologies and objectives:\n",
    "\n",
    "1. **DeepSeek-R1-Zero**: This model was trained entirely through reinforcement learning (RL) without any supervised fine-tuning (SFT). While it developed strong reasoning capabilities, it faced challenges such as less readable outputs and occasional mixing of languages within responses, making it less practical for real-world applications. \n",
    "\n",
    "\n",
    "2. **DeepSeek-R1**: To address the limitations of R1-Zero, DeepSeek-R1 was developed using a hybrid approach that combines reinforcement learning with supervised fine-tuning. This method incorporated curated datasets to improve the model's readability and coherence, effectively reducing issues like language mixing and fragmented reasoning. As a result, DeepSeek-R1 is more suitable for practical use. \n",
    "\n",
    "\n",
    "3. **DeepSeek-R1 Distilled Models**: These are smaller, more efficient versions of the original DeepSeek-R1 model, created through a process called distillation. Distillation involves training a compact model to replicate the behavior of a larger model, thereby retaining much of its reasoning power while reducing computational demands. DeepSeek has released several distilled models based on different architectures, such as Qwen and Llama, with varying parameter sizes (e.g., 1.5B, 7B, 14B, 32B, and 70B). These distilled models offer a balance between performance and resource efficiency, making them accessible for a wider range of applications. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e90891c3-7f38-4ff6-9e0e-765d74da014e",
   "metadata": {},
   "source": [
    "The table below captures the DeepSeek R1 non-distilled model variants,\n",
    "\n",
    "| **Model** | **#Total Params** | **#Activated Params** | **Context Length** | **Download** | **Suggested Instances for Hosting** |\n",
    "| :------------: | :------------: | :------------: | :------------: | :------------: | :------------: |\n",
    "| DeepSeek-R1-Zero | 671B | 37B | 128K   | [ðŸ¤— HuggingFace](https://huggingface.co/deepseek-ai/DeepSeek-R1-Zero)   | `ml.p5.48xlarge`, `p5e.48xlarge` |\n",
    "| DeepSeek-R1   | 671B | 37B |  128K   | [ðŸ¤— HuggingFace](https://huggingface.co/deepseek-ai/DeepSeek-R1)   | `ml.p5.48xlarge`, `p5e.48xlarge` |\n",
    "\n",
    "The table below captures the DeepSeek R1 distilled model variants,\n",
    "\n",
    "| **Model** | **Base Model** | **Download** | **Suggested Instances for Hosting** |\n",
    "| :------------: | :------------: | :------------: | :------------: |\n",
    "| DeepSeek-R1-Distill-Qwen-1.5B  | [Qwen2.5-Math-1.5B](https://huggingface.co/Qwen/Qwen2.5-Math-1.5B) | [ðŸ¤— HuggingFace](https://huggingface.co/deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B)   | `ml.g4dn.xlarge`, `ml.g5.xlarge`, `ml.g6.xlarge`, `ml.g6e.xlarge`   |\n",
    "| DeepSeek-R1-Distill-Qwen-7B  | [Qwen2.5-Math-7B](https://huggingface.co/Qwen/Qwen2.5-Math-7B) | [ðŸ¤— HuggingFace](https://huggingface.co/deepseek-ai/DeepSeek-R1-Distill-Qwen-7B)   | `ml.g5.2xlarge`, `ml.g6.2xlarge`, `ml.g6e.2xlarge` |\n",
    "| DeepSeek-R1-Distill-Llama-8B  | [Llama-3.1-8B](https://huggingface.co/meta-llama/Llama-3.1-8B) | [ðŸ¤— HuggingFace](https://huggingface.co/deepseek-ai/DeepSeek-R1-Distill-Llama-8B)   | `ml.g5.2xlarge`, `ml.g6.2xlarge`, `ml.g6e.2xlarge`   |\n",
    "| DeepSeek-R1-Distill-Qwen-14B   | [Qwen2.5-14B](https://huggingface.co/Qwen/Qwen2.5-14B) | [ðŸ¤— HuggingFace](https://huggingface.co/deepseek-ai/DeepSeek-R1-Distill-Qwen-14B)   | `ml.g4dn.12xlarge`, `ml.g5.12xlarge`, `ml.g6.12xlarge`, `ml.g6e.12xlarge`   |\n",
    "| DeepSeek-R1-Distill-Qwen-32B  | [Qwen2.5-32B](https://huggingface.co/Qwen/Qwen2.5-32B) | [ðŸ¤— HuggingFace](https://huggingface.co/deepseek-ai/DeepSeek-R1-Distill-Qwen-32B)   | `ml.g4dn.12xlarge`, `ml.g5.12xlarge`, `ml.g6.12xlarge`, `ml.g6e.12xlarge` |\n",
    "| DeepSeek-R1-Distill-Llama-70B  | [Llama-3.3-70B-Instruct](https://huggingface.co/meta-llama/Llama-3.3-70B-Instruct) | [ðŸ¤— HuggingFace](https://huggingface.co/deepseek-ai/DeepSeek-R1-Distill-Llama-70B)   | `ml.g5.48xlarge`, `ml.g6.48xlarge`, `ml.g6e.48xlarge`, `ml.p4d.24xlarge`  |\n",
    "\n",
    "> âš  **Warning:** This is not an exhaustive list of compatible instances, please refer to the SageMaker supported instance list here: https://aws.amazon.com/sagemaker-ai/pricing/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23e8cc62-53a9-4542-a566-56f47ada5e24",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -Uq sagemaker --no-warn-conflicts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "175d9b0b-97af-4e4e-b1ac-426fae5b54c9",
   "metadata": {},
   "source": [
    "## Deploy DeepSeek R1 Distilled Variants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a4f9750-ff90-4985-a978-52b22dbab4fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import sagemaker\n",
    "import boto3\n",
    "from typing import List, Dict\n",
    "from datetime import datetime\n",
    "from sagemaker.huggingface import (\n",
    "    HuggingFaceModel, \n",
    "    get_huggingface_llm_image_uri\n",
    ")\n",
    "from sagemaker.deserializers import JSONDeserializer\n",
    "from sagemaker.serializers import JSONSerializer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe509d4a-91e5-4826-8cd5-8be23ffe875a",
   "metadata": {},
   "outputs": [],
   "source": [
    "boto_region = boto3.Session().region_name\n",
    "session = sagemaker.session.Session(boto_session=boto3.Session(region_name=boto_region))\n",
    "role = sagemaker.get_execution_role()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf03803d-3b72-4a27-861c-2af12106fa18",
   "metadata": {},
   "source": [
    "## Deploy using DJL-Inference Container"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f92ece7-9d1f-4ffd-a056-583e4a0222cc",
   "metadata": {},
   "source": [
    "The [Deep Java Library (DJL) Large Model Inference (LMI)](https://docs.aws.amazon.com/sagemaker/latest/dg/large-model-inference-container-docs.html) containers are specialized Docker containers designed to facilitate the deployment of large language models (LLMs) on Amazon SageMaker. These containers integrate a model server with optimized inference libraries, providing a comprehensive solution for serving LLMs. \n",
    "\n",
    "**Key Features of DJL LMI Containers:**\n",
    "\n",
    "* __Optimized Inference Performance__: Support for popular model architectures like DeepSeek, Mistral, Llama, Falcon and many more..\n",
    "* __Integration with Inference Libraries__: Seamless integration with libraries such as vLLM, TensorRT-LLM, and Transformers NeuronX.\n",
    "* __Advanced Capabilities__: Features like continuous batching, token streaming, quantization (e.g., AWQ, GPTQ, FP8), multi-GPU inference using tensor parallelism, and support for LoRA fine-tuned models.\n",
    "\n",
    "**Benefits for Deploying LLMs with DJL-LMI on Amazon SageMaker:**\n",
    "\n",
    "* __Simplified Deployment__: DJL LMI containers offer a low-code interface, allowing users to specify configurations like model parallelization and optimization settings through a configuration file. \n",
    "* __Performance Optimization__: By leveraging optimized inference libraries and techniques, these containers enhance inference performance, reducing latency and improving throughput.\n",
    "* __Scalability__: Designed to handle large models that may not fit on a single accelerator, enabling efficient scaling across multiple GPUs or specialized hardware like AWS Inferentia."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70b78106-b50f-4a26-afda-d0a865d3a277",
   "metadata": {},
   "outputs": [],
   "source": [
    "## You can get inference image uri programmatically using sagemaker.image_uris.retrieve\n",
    "# deepspeed_image_uri = sagemaker.image_uris.retrieve(\n",
    "#     framework=\"djl-lmi\", \n",
    "#     region=boto_region, \n",
    "#     version=\"0.31.0\"\n",
    "# )\n",
    "djllmi_inference_image_uri = \"763104351884.dkr.ecr.us-east-1.amazonaws.com/djl-inference:0.31.0-lmi13.0.0-cu124\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1121de3-6465-4bcb-a026-9e7b896cc261",
   "metadata": {},
   "source": [
    "Choose an appropriate model name and endpoint name when hosting your model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e01dabec-ad96-4476-8da8-4028983b8a38",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name_lmi = f\"deepseek-r1-distil-llama8b-lmi-{datetime.now().strftime('%y%m%d-%H%M%S')}\"\n",
    "endpoint_name_lmi = f\"{model_name_lmi}-ep\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "976098f1-4cdc-4405-8022-07432ced6d1b",
   "metadata": {},
   "source": [
    "Create a new [SageMaker Model](https://sagemaker.readthedocs.io/en/stable/api/inference/model.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e65d9f4",
   "metadata": {},
   "source": [
    "> âš  Swap `HF_MODEL_ID: deepseek-ai/DeepSeek-R1-Distill-Llama-8B` with another DeepSeek Distilled Variant if you prefer to deploy a different dense model. Optionally, you can include `HF_TOKEN: \"hf_...\"` for gated models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd7e15a5-5897-4187-b22d-20ff0d94ed60",
   "metadata": {},
   "outputs": [],
   "source": [
    "deepseek_lmi_model = sagemaker.Model(\n",
    "    image_uri=djllmi_inference_image_uri,\n",
    "    env={\n",
    "        \"HF_MODEL_ID\": \"deepseek-ai/DeepSeek-R1-Distill-Llama-8B\",\n",
    "        \"OPTION_MAX_MODEL_LEN\": \"10000\",\n",
    "        \"OPTION_GPU_MEMORY_UTILIZATION\": \"0.95\",\n",
    "        \"OPTION_ENABLE_STREAMING\": \"false\",\n",
    "        \"OPTION_ROLLING_BATCH\": \"auto\",\n",
    "        \"OPTION_MODEL_LOADING_TIMEOUT\": \"3600\",\n",
    "        \"OPTION_PAGED_ATTENTION\": \"false\",\n",
    "        \"OPTION_DTYPE\": \"fp16\",\n",
    "    },\n",
    "    role=role,\n",
    "    name=model_name_lmi,\n",
    "    sagemaker_session=sagemaker.Session()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d1ac41f-8586-491b-8bcc-621b527d1f0c",
   "metadata": {},
   "source": [
    "ðŸš€ Deploy. Please wait for the endpoint to be `InService` before running inference against it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "192bb6a4-76c5-4ab7-93a6-5d080a391a9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained_lmi_predictor = deepseek_lmi_model.deploy(\n",
    "    endpoint_name=endpoint_name_lmi,\n",
    "    initial_instance_count=1,\n",
    "    instance_type=\"ml.g5.2xlarge\",\n",
    "    container_startup_health_check_timeout=600,\n",
    "    #wait=False\n",
    ")\n",
    "print(f\"Your DJL-LMI Model Endpoint: {endpoint_name_lmi} is now deployed! ðŸš€\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "481d9178-dade-44c4-b633-99f500b20de6",
   "metadata": {},
   "source": [
    "### Inference with SageMaker SDK"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cc27b2d-78a0-42a0-8605-8c94d4e5fd89",
   "metadata": {},
   "source": [
    "SageMaker python sdk simplifies the inference construct using `sagemaker.Predictor` method."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb2d357e-d4e3-4297-a4cd-6dc45d12630e",
   "metadata": {},
   "source": [
    "`DeepSeek Llama8b` variant is based on 3.1 Llama8b prompt format which is as shown below,\n",
    "\n",
    "\n",
    "```json\n",
    "<|begin_of_text|>\n",
    "<|start_header_id|>system<|end_header_id|>\n",
    "\n",
    "Cutting Knowledge Date: December 2024\n",
    "Today Date: 29 Jan 2025\n",
    "\n",
    "You are a helpful assistant that thinks and reasons before answering.\n",
    "\n",
    "<|eot_id|>\n",
    "<|start_header_id|>user<|end_header_id|>\n",
    "How many R are in STRAWBERRY? Keep your answer and explanation short!\n",
    "<|eot_id|>\n",
    "\n",
    "<|start_header_id|>assistant<|end_header_id|>\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c41b8c10-250a-41ba-abbd-9f9d289e9718",
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained_lmi_predictor = sagemaker.Predictor(\n",
    "     endpoint_name=endpoint_name_lmi,\n",
    "     sagemaker_session=session,\n",
    "     serializer=JSONSerializer(),\n",
    "     deserializer=JSONDeserializer(),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d0096cd-5ccb-4885-85b4-d986ce7711ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_messages(messages: List[Dict[str, str]]) -> List[str]:\n",
    "    \"\"\"\n",
    "    Format messages for Llama 3+ chat models.\n",
    "    \n",
    "    The model only supports 'system', 'user' and 'assistant' roles, starting with 'system', then 'user' and \n",
    "    alternating (u/a/u/a/u...). The last message must be from 'user'.\n",
    "    \"\"\"\n",
    "    # auto assistant suffix\n",
    "    # messages.append({\"role\": \"assistant\"})\n",
    "    \n",
    "    output = \"<|begin_of_text|>\"\n",
    "    # Adding an inferred prefix\n",
    "    system_prefix = f\"\\n\\nCutting Knowledge Date: December 2024\\nToday Date: {datetime.now().strftime('%d %b %Y')}\\n\\n\"\n",
    "    for i, entry in enumerate(messages):\n",
    "        output += f\"<|start_header_id|>{entry['role']}<|end_header_id|>\"\n",
    "        if entry['role'] == 'system':\n",
    "            output += f\"{system_prefix}{entry['content']}<|eot_id|>\"\n",
    "        elif entry['role'] != 'system' and 'content' in entry:\n",
    "            output += f\"\\n\\n{entry['content']}<|eot_id|>\"\n",
    "    output += \"<|start_header_id|>assistant<|end_header_id|>\\n\"\n",
    "    return output\n",
    "\n",
    "\n",
    "# pretrained_lmi_predictor = sagemaker.Predictor(\n",
    "#     endpoint_name=endpoint_name_lmi,\n",
    "#     sagemaker_session=session,\n",
    "#     serializer=JSONSerializer(),\n",
    "#     deserializer=JSONDeserializer(),\n",
    "# )\n",
    "\n",
    "\n",
    "def send_prompt(messages, parameters):\n",
    "    # convert u/a format \n",
    "    frmt_input = format_messages(messages)\n",
    "    payload = {\n",
    "        \"inputs\": frmt_input,\n",
    "        \"parameters\": parameters\n",
    "    }\n",
    "    response = pretrained_lmi_predictor.predict(payload)\n",
    "    return response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26bcf7f7-8f5e-4468-ae53-343ccba7c586",
   "metadata": {},
   "source": [
    "We can continue to use a simple `List[Dict[str, str]]` format to chat and simplify `system`, `user` and `assistant` chat transcripts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c49b7ec8-72b2-4ab7-a93a-0d0e4c2dfa74",
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant that thinks and reasons before answering.\"},\n",
    "    {\"role\": \"user\", \"content\": \"How many R are in STRAWBERRY? Keep your answer and explanation short!\"}\n",
    "]\n",
    "response_deepseek_lmi = send_prompt(\n",
    "    messages, \n",
    "    parameters={\n",
    "        \"temperature\": 0.6, \n",
    "        \"max_new_tokens\": 512\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93ef576e-e03f-47f3-914b-f302642891fc",
   "metadata": {},
   "source": [
    "Simply print your response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b160e750-d509-4486-84c2-dc3fcbc193db",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(response_deepseek_lmi['generated_text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31de0812-e139-4176-92ce-1dc8c33f40c5",
   "metadata": {},
   "source": [
    "## Deploy using HuggingFace TGI Container"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a89707ad-bfbd-47eb-98f1-5784acfc2226",
   "metadata": {},
   "source": [
    "Hugging Face Large Language Model (LLM) Inference Deep Learning Container (DLC) on Amazon SageMaker enables developers to efficiently deploy and serve open-source LLMs at scale. This DLC is powered by Text Generation Inference (TGI), an open-source, purpose-built solution optimized for high-performance text generation tasks. \n",
    "\n",
    "**Key Features of HuggingFace TGI Containers:**\n",
    "\n",
    "* **Tensor Parallelism**: Distributes computation across multiple GPUs, allowing the deployment of large models that exceed the memory capacity of a single GPU.\n",
    "* **Dynamic Batching**: Aggregates multiple incoming requests into a single batch, enhancing throughput and resource utilization.\n",
    "* **Optimized Transformers Code**: Utilizes advanced techniques like flash-attention to improve inference speed and efficiency for popular model architectures like DeepSeek, Llama, Falcon, Mistal, Mixtral and many more.\n",
    "\n",
    "**Benefits for Deploying LLMs with HuggingFace TGI on Amazon SageMaker:**\n",
    "\n",
    "* **Simplified Deployment**: TGI containers provide a low-code interface, allowing users to specify configurations like model parallelization and optimization settings through straightforward configuration files. \n",
    "* **Performance Optimization**: By leveraging optimized inference libraries and techniques, such as tensor parallelism and dynamic batching, these containers enhance inference performance, reducing latency and improving throughput. \n",
    "* **Scalability**: Designed to handle large models, TGI containers enable efficient scaling across multiple GPUs or specialized hardware like AWS Inferentia, ensuring that even the most demanding models can be deployed effectively. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be607155-f91b-4fb2-ace7-ffd91a5d5ca6",
   "metadata": {},
   "source": [
    "Choose an appropriate model name and endpoint name when hosting your model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8989ad83-2ab5-4e3f-89e9-ee18b7ef8b9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name_tgi = f\"deepseek-r1-distil-llama8b-tgi-{datetime.now().strftime('%y%m%d-%H%M%S')}\"\n",
    "endpoint_name_tgi = f\"{model_name_tgi}-ep\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4db5f036-3e30-404e-a73d-e700e9f42d01",
   "metadata": {},
   "source": [
    "For a more exhaustive list, please refer to this [TGI Release Page](https://github.com/aws/deep-learning-containers/releases?q=tgi+AND+gpu&expanded=true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb3d05f0-a653-4665-a580-60c2047cc04d",
   "metadata": {},
   "outputs": [],
   "source": [
    "tgi_inference_image_uri = get_huggingface_llm_image_uri(\n",
    "     \"huggingface\", \n",
    "     version=\"2.3.1\"\n",
    ")\n",
    "print(f\"Using TGI Image: {tgi_inference_image_uri}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4a43e98-a0cd-47c8-8ab9-92dee8bdb087",
   "metadata": {},
   "source": [
    "Create a new [SageMaker HuggingFaceModel](https://sagemaker.readthedocs.io/en/stable/frameworks/huggingface/sagemaker.huggingface.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e9f2cc5",
   "metadata": {},
   "source": [
    "> âš  Swap `HF_MODEL_ID: deepseek-ai/DeepSeek-R1-Distill-Llama-8B` with another DeepSeek Distilled Variant if you prefer to deploy a different dense model. Optionally, you can include `HF_TOKEN: \"hf_...\"` for gated models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "094fa9c8-a1b5-4373-8aa1-be9ff426df1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "deepseek_tgi_model = HuggingFaceModel(\n",
    "    image_uri=tgi_inference_image_uri,\n",
    "    env={\n",
    "        \"HF_MODEL_ID\": \"deepseek-ai/DeepSeek-R1-Distill-Llama-8B\",\n",
    "        \"ENDPOINT_SERVER_TIMEOUT\": \"3600\",\n",
    "        \"MESSAGES_API_ENABLED\": \"true\",\n",
    "        \"OPTION_ENTRYPOINT\": \"inference.py\",\n",
    "        \"SAGEMAKER_ENV\": \"1\",\n",
    "        \"SAGEMAKER_MODEL_SERVER_WORKERS\": \"1\",\n",
    "        \"SAGEMAKER_PROGRAM\": \"inference.py\",\n",
    "        \"SM_NUM_GPUS\": \"1\",\n",
    "        \"MAX_TOTAL_TOKENS\": \"8192\",\n",
    "        \"MAX_INPUT_TOKENS\": \"7168\",\n",
    "        \"MAX_BATCH_PREFILL_TOKENS\": \"7168\",\n",
    "        \"DTYPE\": \"bfloat16\",\n",
    "        \"PORT\": \"8080\"\n",
    "    },\n",
    "    role=role,\n",
    "    name=model_name_tgi,\n",
    "    sagemaker_session=session\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9da1cc94-bafe-4386-892f-72a73cc5517e",
   "metadata": {},
   "source": [
    "ðŸš€ Deploy. Please wait for the endpoint to be `InService` before running inference against it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a5f6c59-09ac-40dd-aaac-09ed9417b90b",
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained_tgi_predictor = deepseek_tgi_model.deploy(\n",
    "    endpoint_name=endpoint_name_tgi,\n",
    "    initial_instance_count=1,\n",
    "    instance_type=\"ml.g5.2xlarge\",\n",
    "    container_startup_health_check_timeout=600,\n",
    "    #wait=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5950723-93cd-42a4-a4e6-c707df01e74c",
   "metadata": {},
   "source": [
    "### Inference with SageMaker SDK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cb8bfb3-a863-47e6-8da5-d8f151c4bcd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant that thinks and reasons before answering.\"},\n",
    "    {\"role\": \"user\", \"content\": \"How many R are in STRAWBERRY? Keep your answer and explanation short!\"}\n",
    "]\n",
    "\n",
    "response_deepseek_tgi = pretrained_tgi_predictor.predict(\n",
    "    {\n",
    "        \"messages\": messages,\n",
    "        \"max_tokens\": 1024,\n",
    "        \"temperature\": 0.6\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3570bf2d-ddff-47c6-9e19-d54d1662bdf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(response_deepseek_tgi[\"choices\"][0][\"message\"][\"content\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9bdcd4e-cef5-464e-b07a-e9a50c414267",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
