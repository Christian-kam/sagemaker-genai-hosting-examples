{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "993dcf59-dd85-4a31-a8cc-02c5d4be5dfe",
   "metadata": {},
   "source": [
    "# Deploy DeepSeek-R1-0528 models on Amazon SageMaker Endpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "718c67ba-30c7-421d-8502-c21634c6a8df",
   "metadata": {},
   "source": [
    "The DeepSeek R1 model has undergone a minor version upgrade, with the current version being DeepSeek-R1-0528. In the latest update, DeepSeek R1 has significantly improved its depth of reasoning and inference capabilities by leveraging increased computational resources and introducing algorithmic optimization mechanisms during post-training. The model has demonstrated outstanding performance across various benchmark evaluations, including mathematics, programming, and general logic. Its overall performance is now approaching that of leading models, such as O3 and Gemini 2.5 Pro.\n",
    "\n",
    "Compared to the previous version, the upgraded model shows significant improvements in handling complex reasoning tasks. For instance, in the AIME 2025 test, the model’s accuracy has increased from 70% in the previous version to 87.5% in the current version. This advancement stems from enhanced thinking depth during the reasoning process: in the AIME test set, the previous model used an average of 12K tokens per question, whereas the new version averages 23K tokens per question.\n",
    "\n",
    "See [DeepSeek Blog](https://api-docs.deepseek.com/news/news250528) for more details"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1649adc6-4e2d-4ebe-badb-aa47ee4c48f4",
   "metadata": {},
   "source": [
    "## Environment Setup\n",
    "\n",
    "First, we'll upgrade the SageMaker SDK to ensure compatibility with the latest features, particularly those needed for large language model deployment and streaming inference.\n",
    "\n",
    "> **Note**: The `--quiet` and `--no-warn-conflicts` flags are used to minimize unnecessary output while installing dependencies.\n",
    "\n",
    "> ⚠️ **Important**: After running the installation cell below, you may need to restart your notebook kernel to ensure the updated packages are properly loaded. To do this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c01a4b2-b0d3-420c-8335-4efd984bb5dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install --upgrade --quiet --no-warn-conflicts sagemaker sagemaker-core python-dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9183bf15-3f3f-4d70-91da-fe269ff421b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import time\n",
    "import json\n",
    "from datetime import datetime\n",
    "from IPython.display import clear_output\n",
    "import boto3\n",
    "import sagemaker\n",
    "\n",
    "role = sagemaker.get_execution_role()  # execution role for the endpoint\n",
    "sess = sagemaker.session.Session()  # sagemaker session for interacting with different AWS APIs\n",
    "bucket = sess.default_bucket()  # bucket to house artifacts\n",
    "region = sess._region_name  # region name of the current SageMaker Studio environment\n",
    "\n",
    "smr_client = boto3.client(\"sagemaker-runtime\")\n",
    "\n",
    "print(f\"sagemaker role arn: {role}\")\n",
    "print(f\"sagemaker bucket: {bucket}\")\n",
    "print(f\"sagemaker session region: {region}\")\n",
    "print(f\"sagemaker version: {sagemaker.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ebdb7fb-319f-4db9-92ea-0c7a94212365",
   "metadata": {},
   "source": [
    "## Configure Model Container and Instance\n",
    "\n",
    "For deploying DeepSeek-R1, we'll use:\n",
    "- **LMI (Deep Java Library) Inference Container**: A container optimized for large language model inference\n",
    "- **P5 Instance**: AWS's latest GPU instance type optimized for large model inference\n",
    "\n",
    "Key configurations:\n",
    "- The container URI points to the DJL inference container in ECR (Elastic Container Registry)\n",
    "- We use `ml.p5en.48xlarge` instance which offer:\n",
    "  - 8 NVIDIA H200 GPUs\n",
    "  - 1128 GB of memory\n",
    "  - High network bandwidth for optimal inference performance\n",
    "\n",
    "> **Note**: The region in the container URI should match your AWS region. Replace `us-east-1` with your region if different."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c04fa2a0-9dc2-439d-91f3-521b3305cd34",
   "metadata": {},
   "outputs": [],
   "source": [
    "CONTAINER_VERSION = \"0.33.0-lmi15.0.0-cu128\"\n",
    "\n",
    "inference_image = f\"763104351884.dkr.ecr.{region}.amazonaws.com/djl-inference:{CONTAINER_VERSION}\"\n",
    "\n",
    "print(f\"Inference image: {inference_image}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1634e71f-a190-408f-bcb9-a314cd057e30",
   "metadata": {},
   "source": [
    "## Deployment of DeepSeek-R1-0528"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1350fac7-a513-4944-b728-df7bb2797720",
   "metadata": {},
   "source": [
    "First, let's deploy full version of the model using latest version of the LMI container. Please see [HugginFace Repo](https://huggingface.co/deepseek-ai/DeepSeek-R1-0528) for more details about the model.\n",
    "\n",
    "Please note you will need access to `ml.p5en.48xlarge` instance to deploy the full version of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cfe1787-fbc1-4705-a2f7-c281736bf9a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "common_env = {\n",
    "    \"HF_MODEL_ID\": \"deepseek-ai/DeepSeek-R1-0528\",\n",
    "}\n",
    "lmi_env = {\n",
    "    \"SERVING_FAIL_FAST\": \"true\",\n",
    "    \"OPTION_ASYNC_MODE\": \"true\",\n",
    "    \"OPTION_ROLLING_BATCH\": \"disable\",\n",
    "    \"OPTION_ENTRYPOINT\": \"djl_python.lmi_vllm.vllm_async_service\",\n",
    "    \"OPTION_TRUST_REMOTE_CODE\": \"True\",\n",
    "    \"OPTION_GPU_MEMORY_UTILIZATION\": \"0.87\",\n",
    "    \"OPTION_MAX_ROLLING_BATCH_SIZE\": \"16\",\n",
    "    \"OPTION_MAX_MODEL_LEN\": \"32768\",\n",
    "    \"OPTION_TENSOR_PARALLEL_DEGREE\": \"max\",\n",
    "}\n",
    "env = common_env | lmi_env\n",
    "model_name = sagemaker.utils.name_from_base(\"model\")\n",
    "endpoint_config_name = model_name\n",
    "endpoint_name = model_name\n",
    "instance_type = \"ml.p5en.48xlarge\"\n",
    "timeout = 3600"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d9a52f1-e90d-4744-ab68-997812e3f826",
   "metadata": {},
   "source": [
    "We are using sagemaker_core SDK for model deployment. \n",
    "\n",
    "This [SDK](https://github.com/aws/sagemaker-core) offers more \"Pythonic\" syntax compared to boto3\n",
    "\n",
    "**Key Features:**\n",
    "- Object-Oriented Interface: Provides a structured way to interact with SageMaker resources, making it easier to manage them using familiar object-oriented programming techniques.\n",
    "- Resource Chaining: Allows seamless connection of SageMaker resources by passing outputs as inputs between them, simplifying workflows and reducing the complexity of parameter management.\n",
    "- Full Parity with SageMaker APIs: Ensures access to all SageMaker capabilities through the SDK, providing a comprehensive toolset for building and deploying machine learning models.\n",
    "- Abstraction of Low-Level Details: Automatically handles resource state transitions and polling logic, freeing developers from managing these intricacies and allowing them to focus on higher-level tasks.\n",
    "- Auto Code Completion: Enhances the developer experience by offering real-time suggestions and completions in popular IDEs, reducing syntax errors and speeding up the coding process.\n",
    "- Comprehensive Documentation and Type Hints: Provides detailed guidance and type hints to help developers understand functionalities, write code faster, and reduce errors without complex API navigation.\n",
    "- Incorporation of Intelligent Defaults: Integrates the previous SageMaker SDK feature of intelligent defaults, allowing developers to set default values for parameters like IAM roles and VPC configurations. This streamlines the setup process, enabling developers to focus on customizations specific to their use case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf216653-f9d3-447b-b79a-7975d5abe5ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker_core.shapes import ContainerDefinition, ProductionVariant\n",
    "from sagemaker_core.resources import Model, EndpointConfig, Endpoint\n",
    "from sagemaker_core.shapes import ProductionVariantRoutingConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b43c0efc-963d-4f1d-a170-7a962c31ff0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model.create(\n",
    "    model_name=model_name,\n",
    "    primary_container=ContainerDefinition(image=inference_image, environment=env),\n",
    "    execution_role_arn=role,\n",
    "    session=sess.boto_session,\n",
    "    region=region,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1338644a-e3ed-46c6-9618-6fceda72f750",
   "metadata": {},
   "outputs": [],
   "source": [
    "endpoint = Endpoint.create(\n",
    "    endpoint_name=endpoint_name,\n",
    "    endpoint_config_name=EndpointConfig.create(\n",
    "        endpoint_config_name=endpoint_config_name,\n",
    "        production_variants=[\n",
    "            ProductionVariant(\n",
    "                variant_name=model_name,\n",
    "                initial_instance_count=1,\n",
    "                instance_type=instance_type,\n",
    "                model_name=model,\n",
    "                container_startup_health_check_timeout_in_seconds=timeout,\n",
    "                model_data_download_timeout_in_seconds=timeout,\n",
    "                routing_config=ProductionVariantRoutingConfig(routing_strategy=\"LEAST_OUTSTANDING_REQUESTS\"),\n",
    "            )\n",
    "        ],\n",
    "    ), \n",
    ")\n",
    "endpoint.wait_for_status(\"InService\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "520be053-b13c-4b7b-be56-cdac055d4d55",
   "metadata": {},
   "source": [
    "## Inference examples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b00164cd-0842-4242-8446-5f2c57ed96df",
   "metadata": {},
   "source": [
    "### Synchronous invocation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f40b2de-e08a-4fc3-990c-1478c7474e6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "payload=json.dumps(\n",
    "    {\n",
    "        \"messages\": [\n",
    "            {\"role\": \"user\", \"content\": \"Name popular places to visit in London?\"}\n",
    "        ],\n",
    "        \"temperature\": 0.9,\n",
    "    }\n",
    ")\n",
    "response = smr_client.invoke_endpoint(EndpointName=endpoint_name, ContentType='application/json', Body=payload)\n",
    "result = json.loads(response['Body'].read().decode(\"utf8\"))\n",
    "print(result[\"choices\"][0][\"message\"][\"content\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78e26058-22d2-47af-b3fd-7d71bd08d9e1",
   "metadata": {},
   "source": [
    "### Asynchronous invocation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18e33b46-53a8-4e6d-8d52-76a553193a41",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LineIterator:\n",
    "    def __init__(self, stream):\n",
    "        self.byte_iterator = iter(stream)\n",
    "        self.buffer = io.BytesIO()\n",
    "        self.read_pos = 0\n",
    "\n",
    "    def __iter__(self):\n",
    "        return self\n",
    "\n",
    "    def __next__(self):\n",
    "        while True:\n",
    "            self.buffer.seek(self.read_pos)\n",
    "            line = self.buffer.readline()\n",
    "            if line and line[-1] == ord(\"\\n\"):\n",
    "                self.read_pos += len(line)\n",
    "                return line[:-1]\n",
    "            try:\n",
    "                chunk = next(self.byte_iterator)\n",
    "            except StopIteration:\n",
    "                if self.read_pos < self.buffer.getbuffer().nbytes:\n",
    "                    continue\n",
    "                raise\n",
    "            if \"PayloadPart\" not in chunk:\n",
    "                print(\"Unknown event type:\" + chunk)\n",
    "                continue\n",
    "            self.buffer.seek(0, io.SEEK_END)\n",
    "            self.buffer.write(chunk[\"PayloadPart\"][\"Bytes\"])\n",
    "\n",
    "def stream_response(endpoint_name, inputs, max_tokens=8189, temperature=0.7, top_p=0.9):    \n",
    "    body = {\n",
    "      \"messages\": [\n",
    "        {\"role\": \"user\", \"content\": [{\"type\": \"text\", \"text\": inputs}]}\n",
    "        ],\n",
    "        \"max_tokens\": max_tokens,\n",
    "        \"temperature\": temperature,\n",
    "        \"top_p\": top_p,\n",
    "        \"stream\": True,\n",
    "    }\n",
    "\n",
    "    resp = smr_client.invoke_endpoint_with_response_stream(\n",
    "        EndpointName=endpoint_name,\n",
    "        Body=json.dumps(body),\n",
    "        ContentType=\"application/json\",\n",
    "    )\n",
    "\n",
    "    event_stream = resp[\"Body\"]\n",
    "    start_json = b\"{\"\n",
    "    full_response = \"\"\n",
    "    start_time = time.time()\n",
    "    token_count = 0\n",
    "\n",
    "    for line in LineIterator(event_stream):\n",
    "        if line != b\"\" and start_json in line:\n",
    "            data = json.loads(line[line.find(start_json):].decode(\"utf-8\"))\n",
    "            token_text = data['choices'][0]['delta'].get('content', '')\n",
    "            full_response += token_text\n",
    "            token_count += 1\n",
    "\n",
    "            # Calculate tokens per second\n",
    "            elapsed_time = time.time() - start_time\n",
    "            tps = token_count / elapsed_time if elapsed_time > 0 else 0\n",
    "\n",
    "            # Clear the output and reprint everything\n",
    "            clear_output(wait=True)\n",
    "            print(full_response)\n",
    "            print(f\"\\nTokens per Second: {tps:.2f}\", end=\"\")\n",
    "\n",
    "    print(\"\\n\") # Add a newline after response is complete\n",
    "    \n",
    "    return full_response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "017e0db2-4f50-4307-981e-4c59dea3e9df",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = \"What is greater 9.11 or 9.8?\"\n",
    "output = stream_response(endpoint_name, inputs, max_tokens=8000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6549f6dc-8a52-47f3-9c06-cf32a26199d8",
   "metadata": {},
   "source": [
    "## Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b152277a-9f1d-4f10-b561-0ab30d54a421",
   "metadata": {},
   "outputs": [],
   "source": [
    "sess.delete_endpoint(endpoint_name)\n",
    "sess.delete_endpoint_config(endpoint_config_name)\n",
    "sess.delete_model(model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abe25bc7-5341-4f3c-9614-0e254bca7147",
   "metadata": {},
   "source": [
    "## Deployment of DeepSeek-R1-0528-Qwen3-8B"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82e73f83-f05b-42ea-be3c-116838f2c15c",
   "metadata": {},
   "source": [
    "Now, let's deploy distilled version of the model (Qwen3-8B) using CloudFormation template. Please see [HugginFace Repo](https://huggingface.co/deepseek-ai/DeepSeek-R1-0528-Qwen3-8B) for more details about the model.\n",
    "\n",
    "Please note you will need access to `ml.g6e.4xlarge` instance to deploy the full version of the model and Sagemaker role should have priviledges to deploy CloudFormation stacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ce29b55-9f6f-4156-b4f0-daf85a92c45e",
   "metadata": {},
   "outputs": [],
   "source": [
    "stack_timestamp = datetime.now().isoformat(timespec=\"seconds\").replace(\":\", \"-\")\n",
    "stack_name = f\"deepseek-r1-0528-{stack_timestamp}\"\n",
    "print(\"stack_name:\", stack_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a96514c-5c39-44c6-97d1-5c14a8ece235",
   "metadata": {},
   "outputs": [],
   "source": [
    "cloudformation = boto3.client(\"cloudformation\")\n",
    "model_name = sagemaker.utils.name_from_base(\"model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc373bfa-6295-4c52-9a62-f435fc1fdc73",
   "metadata": {},
   "source": [
    "We will define string variable that holds CloudFormation template. \n",
    "\n",
    "You can save it to the actual yaml-file and use outside of this notebook example.\n",
    "\n",
    "Please refer to [CloudFormation documentation](https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/Welcome.html) for more details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a81b5f6-d34d-4c2b-8808-a8779de7a60a",
   "metadata": {},
   "outputs": [],
   "source": [
    "cfn_deploy = f\"\"\"\n",
    "AWSTemplateFormatVersion: 2010-09-09\n",
    "\n",
    "Parameters:\n",
    "\n",
    "  DockerImageArn:\n",
    "    Type: String\n",
    "    Default: {inference_image}\n",
    "  RoleArn:\n",
    "    Type: String\n",
    "    Default: {role}\n",
    "  ModelName:\n",
    "    Type: String\n",
    "    Default: {model_name}\n",
    "  InstanceType:\n",
    "    Type: String\n",
    "    Default: ml.g6e.4xlarge\n",
    "  InitialInstanceCount:\n",
    "    Type: Number\n",
    "    Default: 1\n",
    "\n",
    "\n",
    "Resources:\n",
    "\n",
    "  Model:\n",
    "    Type: \"AWS::SageMaker::Model\"\n",
    "    Properties:\n",
    "      ModelName: !Ref ModelName\n",
    "      Containers:\n",
    "        -\n",
    "          ContainerHostname: 'GenericContainer'\n",
    "          Image: !Ref DockerImageArn\n",
    "          Environment:\n",
    "            HF_MODEL_ID: \"deepseek-ai/DeepSeek-R1-0528-Qwen3-8B\"\n",
    "            SERVING_FAIL_FAST: \"True\"\n",
    "            OPTION_ASYNC_MODE: \"True\"\n",
    "            OPTION_ROLLING_BATCH: \"disable\"\n",
    "            OPTION_ENTRYPOINT: \"djl_python.lmi_vllm.vllm_async_service\"\n",
    "            OPTION_TRUST_REMOTE_CODE: \"True\"\n",
    "            OPTION_MAX_ROLLING_BATCH_SIZE: 16\n",
    "            OPTION_MAX_MODEL_LEN: 32768\n",
    "            OPTION_TENSOR_PARALLEL_DEGREE: \"max\"\n",
    "      ExecutionRoleArn: !Ref RoleArn\n",
    "\n",
    "  EndpointConfig:\n",
    "    Type: \"AWS::SageMaker::EndpointConfig\"\n",
    "    Properties:\n",
    "      EndpointConfigName: !GetAtt Model.ModelName\n",
    "      ProductionVariants:\n",
    "        - ModelName: !GetAtt Model.ModelName\n",
    "          VariantName: default\n",
    "          InitialInstanceCount: !Ref InitialInstanceCount\n",
    "          InstanceType: !Ref InstanceType\n",
    "          ModelDataDownloadTimeoutInSeconds: 600\n",
    "          ContainerStartupHealthCheckTimeoutInSeconds: 600\n",
    "\n",
    "  GenericPredictEndpoint:\n",
    "    Type: \"AWS::SageMaker::Endpoint\"\n",
    "    Properties:\n",
    "      EndpointConfigName: !GetAtt EndpointConfig.EndpointConfigName\n",
    "      EndpointName: !GetAtt Model.ModelName\n",
    "\n",
    "Outputs:\n",
    "  EndpointArn:\n",
    "    Value: !Ref GenericPredictEndpoint\n",
    "  EndpointName:\n",
    "    Value: !GetAtt GenericPredictEndpoint.EndpointName\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec40f199-503b-4421-8448-392fe8c81906",
   "metadata": {},
   "outputs": [],
   "source": [
    "stack_parameters = {\n",
    "    \"DockerImageArn\": inference_image,\n",
    "    \"RoleArn\": role,\n",
    "    \"ModelName\": model_name,\n",
    "}\n",
    "\n",
    "stack_parameters = [\n",
    "    {'ParameterKey': key, 'ParameterValue': str(value)}\n",
    "    for key, value in stack_parameters.items()\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6d4a186-e97e-4256-bcd6-878a13aa5f25",
   "metadata": {},
   "outputs": [],
   "source": [
    "create_result = cloudformation.create_stack(\n",
    "        StackName=stack_name,\n",
    "        TemplateBody=cfn_deploy,\n",
    "        Parameters=stack_parameters,\n",
    "        Capabilities=['CAPABILITY_IAM'],\n",
    "        OnFailure='ROLLBACK'\n",
    ")\n",
    "create_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a039fe5-7fb3-4572-a22a-e32076ac1d5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "create_start = datetime.now()\n",
    "\n",
    "print(f'\\nWaiting for {stack_name} stack to be in service...')\n",
    "\n",
    "waiter = cloudformation.get_waiter('stack_create_complete')\n",
    "waiter.wait(StackName = stack_name)\n",
    "\n",
    "print(f\"Creation of stack {stack_name} took {datetime.now() - create_start}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce221cff-3233-4db6-b15c-31b2087bf55f",
   "metadata": {},
   "outputs": [],
   "source": [
    "resources = cloudformation.describe_stack_resources(StackName=stack_name)\n",
    "endpoint_arn = next(r[\"PhysicalResourceId\"] for r in resources[\"StackResources\"] if r[\"LogicalResourceId\"] == \"GenericPredictEndpoint\")\n",
    "endpoint_name = endpoint_arn.split(\"/\")[-1]\n",
    "endpoint_name"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fa3d43d-49a7-46a7-9fa3-b6ebddc1c7b8",
   "metadata": {},
   "source": [
    "## Inference examples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c9270ed-cc96-424a-be49-609af91612a1",
   "metadata": {},
   "source": [
    "### Synchronous invocation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68a9cdea-4700-4e38-84ab-237de9a4db83",
   "metadata": {},
   "outputs": [],
   "source": [
    "payload=json.dumps(\n",
    "    {\n",
    "        \"messages\": [\n",
    "            {\"role\": \"user\", \"content\": \"Name popular places to visit in Italy?\"}\n",
    "        ],\n",
    "        \"temperature\": 0.9,\n",
    "    }\n",
    ")\n",
    "response = smr_client.invoke_endpoint(EndpointName=endpoint_name, ContentType='application/json', Body=payload)\n",
    "result = json.loads(response['Body'].read().decode(\"utf8\"))\n",
    "print(result[\"choices\"][0][\"message\"][\"content\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc6554bd-e22d-4544-9e99-33441a4f8876",
   "metadata": {},
   "source": [
    "### Asynchronous invocation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0382096e-943d-43ce-89d3-00a8ccd60b84",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = \"How many R in a word 'strawberry'?\"\n",
    "output = stream_response(endpoint_name, inputs, max_tokens=8000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9200f1c-a78d-43bd-851e-1304887102a4",
   "metadata": {},
   "source": [
    "## Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65cf9d22-a1e1-4459-8a44-0a98348fca12",
   "metadata": {},
   "outputs": [],
   "source": [
    "cloudformation.delete_stack(StackName=stack_name)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
