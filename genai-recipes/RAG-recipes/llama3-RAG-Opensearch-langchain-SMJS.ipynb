{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e3eed142-6144-4ba2-a693-2bcfdeeae823",
   "metadata": {},
   "source": [
    "# Retrieval Augmented Question (RAG) with Llama3-8B-Instruct on SageMaker JumpStart and Amazon OpenSearch using LangChain\n",
    "\n",
    "RAG Application use cases with Llama3-8B, BGE Large embedding model on SageMaker and OpenSearch as Vector Database"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c7a66db-4d22-4e0f-883c-8e8daf6a4291",
   "metadata": {},
   "source": [
    "In this notebook, we demonstrate the use of [Llama3-8B-Instruct](https://huggingface.co/meta-llama/Meta-Llama-3-8B-Instruct) text generation combined with [BGE Large En v1.5](https://huggingface.co/BAAI/bge-large-en-v1.5) embedding model to efficiently construct a Retrieval Augmented Generation (RAG) QnA system on a SageMaker Notebook. This notebook, powered by an `ml.t3.medium instance`, enables the deployment of LLMs on [SageMaker JumpStart](https://aws.amazon.com/sagemaker/jumpstart/). These can be called with an API endpoint created by SageMaker, which we then use to build, experiment with, and tune for comparing Advanced RAG application techniques using [LangChain](https://www.langchain.com/). Additionally, we showcase how we can use [OpenSearch](https://aws.amazon.com/opensearch-service/) Vector Engine Embedding store to archive and retrieve embeddings, integrating it into your RAG workflow. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57fd4e9d-3604-4d87-aff9-dd1f27168442",
   "metadata": {},
   "source": [
    "![Architecture](RAG-OpenSearch.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "267f4f95-dfab-4c64-956e-6c29274131d0",
   "metadata": {},
   "source": [
    "## Prerequisites"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d597937-7c3e-42cc-b595-309bd8e5ae28",
   "metadata": {},
   "source": [
    "---\n",
    "This Jupyter Notebook can be run on a t3.medium instance (ml.t3.medium). However, to deploy `Llama3-8B-Instruct` and `BGE Large En v1.5` models, you may need to request a quota increase. \n",
    "\n",
    "To request a quota increase, follow these steps:\n",
    "\n",
    "1. Navigate to the [Service Quotas console](https://console.aws.amazon.com/servicequotas/).\n",
    "2. Choose Amazon SageMaker.\n",
    "3. Review your default quota for the following resources:\n",
    "   - `ml.g5.2xlarge` for endpoint usage. You will need two instances. \n",
    "4. If needed, request a quota increase for these resources."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2277979-97a1-41a6-ac40-0f267326b40a",
   "metadata": {},
   "source": [
    "### Changing instance type\n",
    "---\n",
    "Models are supported on the following instance types:\n",
    "\n",
    " - Llama3-8B Text Generation: `ml.g5.2xlarge`, `ml.g5.4xlarge`, `ml.g5.8xlarge`, `ml.g5.12xlarge`, `ml.g5.24xlarge`, `ml.g5.48xlarge`, and `ml.p4d.24xlarge`\n",
    " - BGE Large En v1.5: `ml.g5.2xlarge`, `ml.c6i.xlarge`,`ml.g5.4xlarge`, `ml.g5.8xlarge`, `ml.p3.2xlarge`, and `ml.g4dn.2xlarge`\n",
    "\n",
    "By default, the JumpStartModel class selects a default instance type available in your region. If you would like to use a different instance type, you can do so by specifying instance type in the JumpStartModel class.\n",
    "\n",
    "`my_model = JumpStartModel(model_id=model_id, instance_type=\"ml.g5.12xlarge\")`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8e19e16-86b3-4e27-94bf-00e2f832605c",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Contents\n",
    "---\n",
    "\n",
    "1. [Requirements](#Requirements)\n",
    "2. [Model Deployment](#Model-Deployment)\n",
    "3. [Setup LangChain](#Setup-LangChain)\n",
    "4. [Data Preparation](#Data-Preparation)\n",
    "5. [Question Answering with LangChain Vector Store Wrapper](#Question-Answering-with-LangChain-Vector-Store-Wrapper)\n",
    "6. [Conclusion](#Conclusion)\n",
    "7. [Clean Up Resources](#Clean-Up-Resources)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b7bb282-2718-4911-a92b-4ef084441239",
   "metadata": {},
   "source": [
    "## Requirements\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d709d071-695d-4102-a8cd-6fba6c4678a3",
   "metadata": {},
   "source": [
    "1. Create an Amazon SageMaker Notebook Instance - [Amazon SageMaker](https://docs.aws.amazon.com/sagemaker/latest/dg/gs-setup-working-env.html)\n",
    "    - For Notebook Instance type, choose `ml.t3.medium`.\n",
    "2. For Select Kernel, choose [conda_python3](https://docs.aws.amazon.com/sagemaker/latest/dg/ex1-prepare.html).\n",
    "3. Install the required packages."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f07d152-2889-4004-8eba-a0d9028708db",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\"> \n",
    "\n",
    "<b>NOTE:\n",
    "\n",
    "- </b> For <a href=\"https://aws.amazon.com/sagemaker/studio/\" target=\"_blank\">Amazon SageMaker Studio</a>, select Kernel \"<span style=\"color:green;\">Python 3 (ipykernel)</span>\".\n",
    "\n",
    "- For <a href=\"https://docs.aws.amazon.com/sagemaker/latest/dg/studio.html\" target=\"_blank\">Amazon SageMaker Studio Classic</a>, select Image \"<span style=\"color:green;\">Base Python 3.0</span>\" and Kernel \"<span style=\"color:green;\">Python 3</span>\".\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c22cf2e-971a-4df9-9e27-1cd7a05d8307",
   "metadata": {},
   "source": [
    "To run this notebook you would need to install the following dependencies:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60fde7eb-a354-4934-9126-b793080328c1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%writefile requirements.txt\n",
    "langchain==0.1.14\n",
    "pypdf==4.1.0\n",
    "opensearch-py==2.8.0\n",
    "requests-aws4auth==1.3.1\n",
    "boto3==1.34.58\n",
    "sqlalchemy==2.0.29\n",
    "deprecated==1.2.15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c54cfea5-782f-49c1-8885-c8fc8955e09e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install -U -r requirements.txt --quiet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce69b7e7-1218-4d42-a8d5-1894fc64d721",
   "metadata": {},
   "source": [
    "if you see this error â†’ ERROR: pip's dependency resolver does not currently take into account all the packages after pip install you can ignore it"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecc2e835",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\"> \n",
    "\n",
    "<b>NOTE:</b>\n",
    "\n",
    "Before proceeding, please verify that you have the correct version of the SQLAlchemy library installed. This notebook requires SQLAlchemy >= 2.0.0.\n",
    "\n",
    "To check your installed SQLAlchemy version, you can run the following code:\n",
    "\n",
    "```python\n",
    "import sqlalchemy\n",
    "print(sqlalchemy.__version__)\n",
    "```\n",
    "\n",
    "If the version displayed is less than 2.0.0, and you have already installed the correct version using `pip`, you may need to \"<span style=\"color:green;\">restart</span>\" or \"<span style=\"color:green;\">shutdown</span>\" the Jupyter Notebook kernel to load the updated library.\n",
    "\n",
    "To restart the kernel, go to the \"Kernel\" menu and select \"Restart Kernel\". If that doesn't work, try shutting down the notebook completely and relaunching it.\n",
    "\n",
    "Restarting or shutting down the kernel will resolve any dependency issues and ensure that the correct SQLAlchemy version is loaded.\n",
    "\n",
    "If you haven't installed SQLAlchemy >= 2.0.0 yet, you can do so by running the following command in your terminal or command prompt:\n",
    "\n",
    "```\n",
    "pip install sqlalchemy>=2.0.29\n",
    "```\n",
    "\n",
    "Once the installation is complete, restart or shutdown the Jupyter Notebook kernel as described above.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd4370c7-8453-478b-aefd-9c861bf7f472",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sqlalchemy\n",
    "print(sqlalchemy.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f89ca85",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import langchain\n",
    "print(langchain.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c4bf9f6-1127-4444-b684-f0c933c6158a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    import sagemaker\n",
    "except ImportError:\n",
    "    !pip install sagemaker --quiet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8193291-1bf2-478e-afff-d6afd33a358b",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Model Deployment\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8705564e-143f-411e-8537-a337255f256f",
   "metadata": {
    "tags": []
   },
   "source": [
    "Deploy `Llama 3 8B Instruct` LLM model on Amazon SageMaker JumpStart:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40c642f3-7aaf-4c37-9071-36a19188e753",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Import the JumpStartModel class from the SageMaker JumpStart library\n",
    "from sagemaker.jumpstart.model import JumpStartModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5be0b3ea-8482-4288-8ed2-e03d7bafcb7f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Specify the model ID for the HuggingFace Llama 3 8b Instruct LLM model\n",
    "model_id = \"meta-textgeneration-llama-3-8b-instruct\"\n",
    "accept_eula = True\n",
    "model = JumpStartModel(model_id=model_id, instance_type=\"ml.g5.4xlarge\")\n",
    "llm_predictor = model.deploy(accept_eula=accept_eula)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e1fdb95-bd4b-445c-b597-755fbd9a432a",
   "metadata": {
    "tags": []
   },
   "source": [
    "Deploy `BGE Large En` embedding model on Amazon SageMaker JumpStart:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13c22418-d962-46b2-8e5e-0bcb74e6ff64",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Specify the model ID for the HuggingFace BGE Large EN Embedding model\n",
    "model_id = \"huggingface-sentencesimilarity-bge-large-en-v1-5\"\n",
    "text_embedding_model = JumpStartModel(model_id=model_id)\n",
    "embedding_predictor = text_embedding_model.deploy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfd8c041-8d3c-4265-bb9b-e4d0dd0bb151",
   "metadata": {},
   "source": [
    "## Setup LangChain\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f757369-4769-442a-87ba-db9bf75a4dfd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import sagemaker\n",
    "\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_community.llms import SagemakerEndpoint\n",
    "from langchain_community.embeddings import SagemakerEndpointEmbeddings\n",
    "from langchain_community.llms.sagemaker_endpoint import LLMContentHandler\n",
    "from langchain_community.embeddings.sagemaker_endpoint import EmbeddingsContentHandler"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61f0256b-6fd8-4e1a-8206-ff4ca6efda72",
   "metadata": {},
   "source": [
    "Get endpoint names from predictors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0e28065-2ccc-4174-98b1-c45fdb83cc6d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sess = sagemaker.session.Session()  # sagemaker session for interacting with different AWS APIs\n",
    "region = sess._region_name\n",
    "# predictor = model.deploy(accept_eula=accept_eula)\n",
    "llm_endpoint_name = llm_predictor.endpoint_name\n",
    "embedding_endpoint_name = embedding_predictor.endpoint_name"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7e593b2-1433-4bef-b217-be724575e4fe",
   "metadata": {
    "tags": []
   },
   "source": [
    "Transform input and output data to proccess API calls for`Llama 3 8B Instruct` on Amazon SageMaker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3616478c-d454-4196-b64c-c9445e28839f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from typing import Dict\n",
    "\n",
    "class Llama38BContentHandler(LLMContentHandler):\n",
    "    content_type = \"application/json\"\n",
    "    accepts = \"application/json\"\n",
    "\n",
    "    def transform_input(self, prompt: str, model_kwargs: dict) -> bytes:\n",
    "        payload = {\n",
    "            \"inputs\": prompt,\n",
    "            \"parameters\": {\n",
    "                \"max_new_tokens\": 1000,\n",
    "                \"top_p\": 0.9,\n",
    "                \"temperature\": 0.6,\n",
    "                \"stop\": [\"<|eot_id|>\"],\n",
    "            },\n",
    "        }\n",
    "        input_str = json.dumps(\n",
    "            payload,\n",
    "        )\n",
    "        #print(input_str)\n",
    "        return input_str.encode(\"utf-8\")\n",
    "\n",
    "    def transform_output(self, output: bytes) -> str:\n",
    "        response_json = json.loads(output.read().decode(\"utf-8\"))\n",
    "        #print(response_json)\n",
    "        content = response_json[\"generated_text\"].strip()\n",
    "        return content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5476939-6f13-4ba0-9bc9-b43a92592282",
   "metadata": {},
   "source": [
    "Instantiate the LLM with SageMaker and LangChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c425b6fe-a572-4725-a1ad-ee1c7bf29db4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Instantiate the content handler for Llama3-8B\n",
    "llama_content_handler = Llama38BContentHandler()\n",
    "\n",
    "# Setup for using the Llama3-8B model with SageMaker Endpoint\n",
    "llm = SagemakerEndpoint(\n",
    "     endpoint_name=llm_endpoint_name,\n",
    "     region_name=region, \n",
    "     model_kwargs={\"max_new_tokens\": 1024, \"top_p\": 0.9, \"temperature\": 0.7},\n",
    "     content_handler=llama_content_handler\n",
    " )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f58be6d-c4ff-4948-babc-52c41228c427",
   "metadata": {},
   "source": [
    "Transform input and output data to proccess API calls for`BGE Large En` on Amazon SageMaker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "410c2cf2-dda6-4617-b828-255b4aa4dd57",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from typing import List\n",
    "\n",
    "class BGEContentHandlerV15(EmbeddingsContentHandler):\n",
    "    content_type = \"application/json\"\n",
    "    accepts = \"application/json\"\n",
    "\n",
    "    def transform_input(self, text_inputs: List[str], model_kwargs: dict) -> bytes:\n",
    "        \"\"\"\n",
    "        Transforms the input into bytes that can be consumed by SageMaker endpoint.\n",
    "        Args:\n",
    "            text_inputs (list[str]): A list of input text strings to be processed.\n",
    "            model_kwargs (Dict): Additional keyword arguments to be passed to the endpoint.\n",
    "               Possible keys and their descriptions:\n",
    "               - mode (str): Inference method. Valid modes are 'embedding', 'nn_corpus', and 'nn_train_data'.\n",
    "               - corpus (str): Corpus for Nearest Neighbor. Required when mode is 'nn_corpus'.\n",
    "               - top_k (int): Top K for Nearest Neighbor. Required when mode is 'nn_corpus'.\n",
    "               - queries (list[str]): Queries for Nearest Neighbor. Required when mode is 'nn_corpus' or 'nn_train_data'.\n",
    "        Returns:\n",
    "            The transformed bytes input.\n",
    "        \"\"\"\n",
    "        input_str = json.dumps(\n",
    "            {\n",
    "                \"text_inputs\": text_inputs,\n",
    "                **model_kwargs\n",
    "            }\n",
    "        )\n",
    "        return input_str.encode(\"utf-8\")\n",
    "\n",
    "    def transform_output(self, output: bytes) -> List[List[float]]:\n",
    "        \"\"\"\n",
    "        Transforms the bytes output from the endpoint into a list of embeddings.\n",
    "        Args:\n",
    "            output: The bytes output from SageMaker endpoint.\n",
    "        Returns:\n",
    "            The transformed output - list of embeddings\n",
    "        Note:\n",
    "            The length of the outer list is the number of input strings.\n",
    "            The length of the inner lists is the embedding dimension.\n",
    "        \"\"\"\n",
    "        response_json = json.loads(output.read().decode(\"utf-8\"))\n",
    "        return response_json[\"embedding\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54c4749e-e437-4212-b69d-6c800aa0e21c",
   "metadata": {},
   "source": [
    "Instantiate the embedding model with SageMaker and LangChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84f6e08e-c969-4a05-8587-ad49f9d9dca5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "bge_content_handler = BGEContentHandlerV15()\n",
    "sagemaker_embeddings = SagemakerEndpointEmbeddings(\n",
    "    endpoint_name=embedding_endpoint_name,\n",
    "    region_name=region,\n",
    "    model_kwargs={\"mode\": \"embedding\"},\n",
    "    content_handler=bge_content_handler,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c88793e5-c562-48ce-858b-50c918ac5249",
   "metadata": {},
   "source": [
    "## Data Preparation\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "433e0dc2-718f-47af-aa60-30fa9a60cae3",
   "metadata": {
    "tags": []
   },
   "source": [
    "Let's first download some of the files to build our document store.\n",
    "\n",
    "In this example, you will use several years of Amazon's Letter to Shareholders as a text corpus to perform Q&A on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8b0cbc6-367a-443a-9e59-c63640a1e4c4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!mkdir -p ./data\n",
    "\n",
    "from urllib.request import urlretrieve\n",
    "urls = [\n",
    "    'https://d18rn0p25nwr6d.cloudfront.net/CIK-0001018724/c7c14359-36fa-40c3-b3ca-5bf7f3fa0b96.pdf',\n",
    "    'https://d18rn0p25nwr6d.cloudfront.net/CIK-0001018724/d2fde7ee-05f7-419d-9ce8-186de4c96e25.pdf',\n",
    "    'https://d18rn0p25nwr6d.cloudfront.net/CIK-0001018724/f965e5c3-fded-45d3-bbdb-f750f156dcc9.pdf',\n",
    "    'https://d18rn0p25nwr6d.cloudfront.net/CIK-0001018724/336d8745-ea82-40a5-9acc-1a89df23d0f3.pdf'\n",
    "]\n",
    "\n",
    "filenames = [\n",
    "    'AMZN-2024-10-K-Annual-Report.pdf',\n",
    "    'AMZN-2023-10-K-Annual-Report.pdf',\n",
    "    'AMZN-2022-10-K-Annual-Report.pdf',\n",
    "    'AMZN-2021-10-K-Annual-Report.pdf'\n",
    "]\n",
    "\n",
    "metadata = [\n",
    "    dict(year=2024, source=filenames[0]),\n",
    "    dict(year=2023, source=filenames[1]),\n",
    "    dict(year=2022, source=filenames[2]),\n",
    "    dict(year=2021, source=filenames[3])]\n",
    "\n",
    "data_root = \"./data/\"\n",
    "\n",
    "for idx, url in enumerate(urls):\n",
    "    file_path = data_root + filenames[idx]\n",
    "    urlretrieve(url, file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "859253bf-4bb0-43bf-999a-e1abb1f6983b",
   "metadata": {},
   "source": [
    "If you take a look into the Amazon 10-Ks, the first 4 pages are all the very similar and may skew the responses if you they are kept in the embeddings. This will cause repetition, take longer to generate embeddings, and may skew your results. In the next section you will take the downloaded data, trim the 10-K (first 4 pages) and overwrite them as processed files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ac21b76-14b4-4c64-8cfe-408d877426c4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pypdf import PdfReader, PdfWriter\n",
    "import glob\n",
    "\n",
    "local_pdfs = glob.glob(data_root + '*.pdf')\n",
    "\n",
    "# Iterate over each PDF file\n",
    "for idx, local_pdf in enumerate(local_pdfs):\n",
    "    pdf_reader = PdfReader(local_pdf)\n",
    "    pdf_writer = PdfWriter()\n",
    "    \n",
    "    if idx == 0:\n",
    "        # Keep the first 4 pages for the first document\n",
    "        for pagenum in range(len(pdf_reader.pages)):\n",
    "            page = pdf_reader.pages[pagenum]\n",
    "            pdf_writer.add_page(page)\n",
    "    else:\n",
    "        # Remove the first 4 pages for other documents\n",
    "        for pagenum in range(4, len(pdf_reader.pages)):\n",
    "            page = pdf_reader.pages[pagenum]\n",
    "            pdf_writer.add_page(page)\n",
    "\n",
    "    # Write the modified content to a new file\n",
    "    with open(local_pdf, 'wb') as new_file:\n",
    "        new_file.seek(0)\n",
    "        pdf_writer.write(new_file)\n",
    "        new_file.truncate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "687fa7ac-6605-4842-87f8-7cc844e01c12",
   "metadata": {},
   "source": [
    "After downloading we can load the documents with the help of [DirectoryLoader from PyPDF available under LangChain](https://python.langchain.com/en/latest/reference/modules/document_loaders.html) and splitting them into smaller chunks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80600818-b41c-45b2-b86b-2e4c69271ed6",
   "metadata": {},
   "source": [
    "Note: The retrieved document/text should be large enough to contain enough information to answer a question; but small enough to fit into the LLM prompt. Also the embeddings model has a limit of the length of input tokens limited to 512 tokens, which roughly translates to ~2000 characters. For the sake of this use-case we are creating chunks of roughly 1000 characters with an overlap of 100 characters using [RecursiveCharacterTextSplitter](https://python.langchain.com/en/latest/modules/indexes/text_splitters/examples/recursive_text_splitter.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f13cbcc0-908c-4fa3-adab-f9eae209cf92",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "documents = []\n",
    "\n",
    "for idx, file in enumerate(filenames):\n",
    "    loader = PyPDFLoader(data_root + file)\n",
    "    document = loader.load()\n",
    "    for document_fragment in document:\n",
    "        document_fragment.metadata = metadata[idx]\n",
    "\n",
    "    documents += document\n",
    "\n",
    "# - in our testing Character split works better with this PDF data set\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    # Set a really small chunk size, just to show.\n",
    "    chunk_size=1000,\n",
    "    chunk_overlap=100,\n",
    ")\n",
    "\n",
    "docs = text_splitter.split_documents(documents)\n",
    "print(docs[100])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e27bc3e-9d2b-47cc-9764-9c8b16200b06",
   "metadata": {},
   "source": [
    "Before we are proceeding we are looking into some interesting statistics regarding the document preprocessing we just performed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1d6183e-9ceb-429c-8042-935d56acf4d3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "avg_doc_length = lambda documents: sum([len(doc.page_content) for doc in documents])//len(documents)\n",
    "\n",
    "print(f'Average length among {len(documents)} documents loaded is {avg_doc_length(documents)} characters.')\n",
    "print(f'After the split we have {len(docs)} documents as opposed to the original {len(documents)}.')\n",
    "print(f'Average length among {len(docs)} documents (after split) is {avg_doc_length(docs)} characters.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d55e72f-6162-4aa5-9aa9-0bbb29b026ea",
   "metadata": {},
   "source": [
    "We had 4 PDF documents which have been split into smaller ~500 chunks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66b550cd-1f0f-445b-9c3b-dbd7abf5294f",
   "metadata": {},
   "source": [
    "Now we can see how a sample embedding would look like for one of those chunks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f67cd64a-ba7a-419e-953a-307704772f57",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sample_embedding = np.array(sagemaker_embeddings.embed_query(docs[0].page_content))\n",
    "print(\"Sample embedding of a document chunk: \", sample_embedding)\n",
    "print(\"Size of the embedding: \", sample_embedding.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "967efb1a-8586-4a74-bbd4-b52d1730693b",
   "metadata": {
    "tags": []
   },
   "source": [
    "We can use [OpenSearch](https://aws.amazon.com/opensearch-service/) implementation with [LangChain](https://python.langchain.com/v0.2/api_reference/community/vectorstores/langchain_community.vectorstores.opensearch_vector_search.OpenSearchVectorSearch.html) to ingest the documents to OpenSearch service."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "375f1ae6-e5c8-4283-a93c-96a57432cbeb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "##Provide the Opensearch url here \n",
    "## Retrieve OpenSearch url by going to AWS Console->Amazon OpenSearch Service-> Click on the Domain you would liek to use. \n",
    "import os\n",
    "aos_url = \"https://<vpc-opensearchservi-xxxxx>:443\"\n",
    "os.environ['OPENSEARCH_URL'] = aos_url\n",
    "region"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1444b67d-63e7-4eaa-a127-671a64380cd7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create a Boto3 session\n",
    "import boto3\n",
    "session = boto3.Session()\n",
    "\n",
    "# Get the account id\n",
    "account_id = boto3.client('sts').get_caller_identity().get('Account')\n",
    "\n",
    "# Get the current region\n",
    "region = session.region_name\n",
    "\n",
    "cfn = boto3.client('cloudformation')\n",
    "\n",
    "# Method to obtain output variables from Cloudformation stack. \n",
    "def get_cfn_outputs(stackname):\n",
    "    outputs = {}\n",
    "    for output in cfn.describe_stacks(StackName=stackname)['Stacks'][0]['Outputs']:\n",
    "        outputs[output['OutputKey']] = output['OutputValue']\n",
    "    return outputs\n",
    "\n",
    "## Setup variables to use for the rest of the demo\n",
    "cloudformation_stack_name = \"rag-opensearch\"\n",
    "\n",
    "outputs = get_cfn_outputs(cloudformation_stack_name)\n",
    "\n",
    "# We will just print all the variables so you can easily copy if needed.\n",
    "# outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2163ecf9-e899-4b75-8c5c-8f282ac7e201",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Connect to OpenSearch using the internal username and password obtained from AWS Secrets Manager\n",
    "from opensearchpy import OpenSearch, RequestsHttpConnection\n",
    "\n",
    "kms = boto3.client('secretsmanager')\n",
    "aos_username = outputs['OpenSearchUsername']\n",
    "aos_password = kms.get_secret_value(SecretId=outputs['OpenSearchPasswordArn'])['SecretString']\n",
    "auth = (aos_username, aos_password)\n",
    "# print(auth)\n",
    "#\"admin\",\"*****\" )\n",
    "    \n",
    "# Create OpenSearch client\n",
    "aos_client = OpenSearch(\n",
    "    hosts=[aos_url],\n",
    "    http_auth=auth,\n",
    "    use_ssl=True,\n",
    "    verify_certs=True,\n",
    "    connection_class=RequestsHttpConnection\n",
    ")\n",
    "# print(f\"Connected to OpenSearch endpoint :{aos_client}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "670ae727-dd20-42e4-b7e9-56d9f20181f6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define the role mapping to grant permissions to  AOS username and notebook_iam_role_arn\n",
    "from sagemaker import get_execution_role\n",
    "\n",
    "\n",
    "sagemaker_execution_role = get_execution_role()\n",
    "print(sagemaker_execution_role)\n",
    "role_name = \"all_access\"\n",
    "\n",
    "role_mapping = {\n",
    "    \"backend_roles\": [sagemaker_execution_role],\n",
    "    \"users\" : [ aos_username]\n",
    "}\n",
    "\n",
    "\n",
    "# Create the role mapping\n",
    "response = aos_client.security.create_role_mapping(role=role_name, body=role_mapping)\n",
    "print(\"Role mapping created:\", response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d27ba0e2-a01c-404c-8d38-15c4dec063a2",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain_community.vectorstores import OpenSearchVectorSearch\n",
    "from opensearchpy import RequestsHttpConnection\n",
    "from requests_aws4auth import AWS4Auth\n",
    "import boto3\n",
    "\n",
    "##Make sure the execution role has permissions for Opensearch\n",
    "service = \"es\"\n",
    "region_4Auth = region\n",
    "credentials = boto3.Session().get_credentials()\n",
    "# print(credentials.access_key)\n",
    "awsauth = AWS4Auth(\n",
    "    credentials.access_key,\n",
    "    credentials.secret_key,\n",
    "    region_4Auth,\n",
    "    service,\n",
    "    session_token=credentials.token\n",
    ")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b926375c-e64a-40da-8b1b-0dda0649aa38",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.indexes.vectorstore import VectorStoreIndexWrapper\n",
    "\n",
    "# Initialize OpenSearchVectorSearch\n",
    "vectorstore_opensearch = OpenSearchVectorSearch.from_documents(\n",
    "    docs,\n",
    "    sagemaker_embeddings,\n",
    "    http_auth=awsauth,  # Auth will use the IAM role\n",
    "    use_ssl=True,\n",
    "    verify_certs=True,\n",
    "    connection_class=RequestsHttpConnection,\n",
    "    bulk_size=2000  # Increase this to accommodate the number of documents you have\n",
    ")\n",
    "\n",
    "# Wrap the OpenSearch vector store with the VectorStoreIndexWrapper\n",
    "wrapper_store_opensearch = VectorStoreIndexWrapper(vectorstore=vectorstore_opensearch)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c95675c9-7116-4bd3-ba63-30963750e36f",
   "metadata": {},
   "source": [
    "## Question Answering with LangChain Vector Store Wrapper\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2d335bb-63bf-4870-8e6e-019a1b7c005d",
   "metadata": {
    "tags": []
   },
   "source": [
    "We use the wrapper provided by LangChain which wraps around the Vector Store and takes input the LLM. This wrapper performs the following steps behind the scences:\n",
    "\n",
    "- Takes input the question\n",
    "- Create question embedding\n",
    "- Fetch relevant documents\n",
    "- Stuff the documents and the question into a prompt\n",
    "- Invoke the model with the prompt and generate the answer in a human readable manner."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cf2ff92-84dd-4ad7-856c-643e342cf5cd",
   "metadata": {},
   "source": [
    "*Note: In this example we are using `Llama 3 8B Instruct` as the LLM under Amazon SageMaker, this particular model performs best if the inputs are provided under `<|begin_of_text|><|start_header_id|>system<|end_header_id|>`, `{{system_message}}`, `<|eot_id|><|start_header_id|>user<|end_header_id|>`, `{{user_message}}`, and the model is requested to generate an output after `<|eot_id|><|start_header_id|>assistant<|end_header_id|>`. In the cell below you see an example of how to control the prompt such that the LLM stays grounded and doesn't answer outside the context.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d71d5188-4caa-414c-a76e-33ae8cec19ad",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "prompt_template = \"\"\"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
    "You are a helpful assistant.\n",
    "<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
    "{query}\n",
    "<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
    "\"\"\"\n",
    "PROMPT = PromptTemplate(\n",
    "    template=prompt_template, input_variables=[\"query\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa490dbc-9abe-4bb6-a26c-26f6d10591ad",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "query = \"How did AWS perform in 2021?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37091fb6-02e7-4601-8210-83129076a87d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "answer = wrapper_store_opensearch.query(question=PROMPT.format(query=query), llm=llm)\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "964bc809-6c6b-4989-8835-1cc7fcc731f7",
   "metadata": {},
   "source": [
    "We can ask another question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d94688dd-2ef4-462e-9186-1a828b02e558",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "query_2 = \"How much square footage did Amazon have in North America in 2023?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac316dbb-94c8-4bb6-872a-6c027df8f9d2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "answer = wrapper_store_opensearch.query(question=PROMPT.format(query=query_2), llm=llm)\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4187b411-b7ce-48e7-bda4-d8d2abcefc53",
   "metadata": {},
   "source": [
    "### Regular Retriever Chain\n",
    "---\n",
    "In the above scenario you explored the quick and easy way to get a context-aware answer to your question. Now let's have a look at a more customizable option with the help of [RetrievalQA](https://docs.smith.langchain.com/cookbook/hub-examples/retrieval-qa-chain) where you can customize how the documents fetched should be added to prompt using `chain_type` parameter. Also, if you want to control how many relevant documents should be retrieved then change the `k` parameter in the cell below to see different outputs. In many scenarios you might want to know which were the source documents that the LLM used to generate the answer, you can get those documents in the output using `return_source_documents` which returns the documents that are added to the context of the LLM prompt. `RetrievalQA` also allows you to provide a custom [prompt template](https://python.langchain.com/docs/modules/model_io/prompts/quick_start/) which can be specific to the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "786b9a12-3407-47ce-8457-437059d84788",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.chains import RetrievalQA\n",
    "\n",
    "prompt_template = \"\"\"\n",
    "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
    "\n",
    "This is a conversation between an AI assistant and a Human.\n",
    "\n",
    "<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
    "\n",
    "Use the following pieces of context to provide a concise answer to the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
    "#### Context ####\n",
    "{context}\n",
    "#### End of Context ####\n",
    "\n",
    "Question: {question}\n",
    "<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
    "\"\"\"\n",
    "PROMPT = PromptTemplate(\n",
    "    template=prompt_template, input_variables=[\"context\", \"question\"]\n",
    ")\n",
    "\n",
    "qa = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    chain_type=\"stuff\",\n",
    "    retriever=vectorstore_opensearch.as_retriever(\n",
    "        search_type=\"similarity\", search_kwargs={\"k\": 3}\n",
    "    ),\n",
    "    return_source_documents=True,\n",
    "    chain_type_kwargs={\"prompt\": PROMPT}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70ff4d62-82a5-4f07-9ed4-828b468b6356",
   "metadata": {},
   "source": [
    "Let's start asking questions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a304e6ab-a8ed-4d85-be9b-35ed60721a0b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "query = \"How did AWS perform in 2023?\"\n",
    "result = qa({\"query\": query})\n",
    "print(result['result'])\n",
    "\n",
    "print(f\"\\n{result['source_documents']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba165d2c-8ee5-40e7-91b5-94be5bdc9fde",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "query = \"What are some of the risk factors associated to Amazon?\"\n",
    "result = qa({\"query\": query})\n",
    "print(result['result'])\n",
    "\n",
    "print(f\"\\n{result['source_documents']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4c4a7f8-6bc5-4ede-bacb-ffcd656e9e74",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "query = \"Was Amazon involved in any lawsuits in 2022? What were they?\"\n",
    "result = qa({\"query\": query})\n",
    "print(result['result'])\n",
    "\n",
    "print(f\"\\n{result['source_documents']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89ff4635-7987-42a2-aea6-b3d90110c7dd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "query = \"What was Amazon's revenue in 2021?\"\n",
    "\n",
    "result = qa({\"query\": query})\n",
    "\n",
    "print(result['result'])\n",
    "\n",
    "print(f\"\\n{result['source_documents']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d52e4c55-0f80-47eb-83f7-6ae28fedf57f",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd32616f-9b97-4960-9ccc-47f20a95dcc6",
   "metadata": {},
   "source": [
    "Congratulations on completing the Retrieval Augmented Generation(RAG) notebook with `Llama3 8b`! Through this notebook, you were able to learn how to leverage the power of `Llama3 8b` with  `LangChain` and `OpenSearch`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e83d260d-8771-41ae-b173-0f81228b28dc",
   "metadata": {},
   "source": [
    "In the above implementation of Advanced RAG based Question Answering we have explored the following concepts and how to implement them using Amazon SageMaker JumpStart and it's LangChain integration."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de5d08cb-6828-4089-a8fc-55c12637a2f1",
   "metadata": {},
   "source": [
    "- Deploying models on Amazon SageMaker JumpStart\n",
    "- Setting up `Llama3-8b` and `BGE Large En v1.5` with LangChain\n",
    "- Loading documents of different kind and generating embeddings to create a vector store\n",
    "- Retrieving documents to the question using the following approaches from LangChain\n",
    "    - Regular Retrieval Chain\n",
    "- Preparing a prompt which goes as input to the LLM\n",
    "- Present an answer in a human friendly manner"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db0319f2-5309-45ba-a15c-2ff7b3720133",
   "metadata": {},
   "source": [
    "### Take-aways\n",
    "---\n",
    "- Experiment with different retrieval techniques\n",
    "- Leverage `Llama3-8b` and `BGE Large En v1.5` models available under Amazon SageMaker JumpStart\n",
    "- Explore options such as persistent storage of embeddings and document chunks\n",
    "- Integration with enterprise data stores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e69e4cb2-5ad3-439c-90ea-f538bc9872e8",
   "metadata": {},
   "source": [
    "## Clean Up Resources\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a99d7aae-56a6-4c63-8cea-9ac73b930eb3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Delete resources\n",
    "llm_predictor.delete_model()\n",
    "llm_predictor.delete_endpoint()\n",
    "embedding_predictor.delete_model()\n",
    "embedding_predictor.delete_endpoint()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaa7dc0f-cf00-4bd7-a20c-8952ef75fa86",
   "metadata": {},
   "source": [
    "# Thank You!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p310",
   "language": "python",
   "name": "conda_pytorch_p310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
