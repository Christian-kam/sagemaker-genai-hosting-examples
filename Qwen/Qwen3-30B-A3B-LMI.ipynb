{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8d7620d7-05d4-49be-9f4c-9dab1e25f17d",
   "metadata": {},
   "source": [
    "# ðŸš€ Deploy Qwen3 30B A3B Model on Amazon SageMaker AI \n",
    "\n",
    "## Introduction: [Qwen3 30B A3B](https://huggingface.co/Qwen/Qwen3-30B-A3B)\n",
    "\n",
    "`Qwen3-30B-A3B` is part of the [latest generation of Qwen language models](https://qwenlm.github.io/blog/qwen3/), featuring a Mixture-of-Experts (MoE) architecture with:\n",
    "\n",
    "- **Total Parameters**: 30.5B parameters\n",
    "- **Activated Parameters**: 3.3B parameters (approximately 10% of total)\n",
    "- **Architecture Details**:\n",
    "    - 48 layers\n",
    "    - 32 attention heads for queries and 4 for key/values (GQA)\n",
    "    - 128 total experts with 8 activated experts\n",
    "    - Native context length of 32,768 tokens (expandable to 131,072 with YaRN)\n",
    "\n",
    "### Key Features\n",
    "1. Hybrid Thinking Modes:\n",
    "\n",
    "- Thinking Mode: Enables step-by-step reasoning for complex problems\n",
    "Non-Thinking Mode: Provides quick responses for simpler queries\n",
    "Seamless switching between modes for optimal performance\n",
    "\n",
    "2. Strong Capabilities:\n",
    "\n",
    "- Advanced reasoning and problem-solving\n",
    "- Excellent instruction following\n",
    "- Enhanced agent capabilities for tool integration\n",
    "- Support for 119+ languages and dialects\n",
    "\n",
    "3. Model Architecture:\n",
    "\n",
    "- MoE architecture enabling efficient parameter usage\n",
    "- Only activates ~10% of parameters during inference\n",
    "- Optimized for both performance and computational efficiency\n",
    "  \n",
    "This model represents a significant advancement in open-source language models, offering competitive performance while maintaining efficient resource utilization through its MoE architecture. It's particularly well-suited for deployment in production environments where both performance and cost efficiency are crucial considerations.\n",
    "Let's get started deploying one of the most capable open-source reasoning models available today!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0495a9c3-93c3-450f-82d7-6d91aa30492d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -Uq sagemaker boto3 huggingface_hub --force-reinstall --no-cache-dir --quiet --no-warn-conflicts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49095ae2-0f51-49e3-acf4-e554fff52cbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import sagemaker\n",
    "import boto3\n",
    "import sys\n",
    "import time\n",
    "from typing import List, Dict\n",
    "from datetime import datetime\n",
    "from sagemaker.huggingface import (\n",
    "    HuggingFaceModel, \n",
    "    get_huggingface_llm_image_uri\n",
    ")\n",
    "\n",
    "boto_region = boto3.Session().region_name\n",
    "sagemaker_session = sagemaker.session.Session(boto_session=boto3.Session(region_name=boto_region))\n",
    "role = sagemaker.get_execution_role()\n",
    "sagemaker_client = boto3.client(\"sagemaker\")\n",
    "sagemaker_runtime_client = boto3.client(\"sagemaker-runtime\")\n",
    "s3_client = boto3.client(\"s3\")\n",
    "\n",
    "model_bucket = sagemaker_session.default_bucket()  # bucket to house artifacts\n",
    "s3_model_prefix = (\n",
    "    \"hf-large-models/model_qwen3\"  # folder within bucket where code artifact will go\n",
    ")\n",
    "prefix = sagemaker.utils.unique_name_from_base(\"DEMO\")\n",
    "print(f\"prefix: {prefix}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66560cdc-9dda-478a-bac2-88ed2e6feef3",
   "metadata": {},
   "source": [
    "## Setup your SageMaker Real-time Endpoint \n",
    "### Create a SageMaker endpoint configuration\n",
    "\n",
    "We begin by creating the endpoint configuration and set MinInstanceCount to 0. This allows the endpoint to scale in all the way down to zero instances when not in use. See the [notebook example for SageMaker AI endpoint scale down to zero](https://github.com/aws-samples/sagemaker-genai-hosting-examples/tree/02236395d44cf54c201eefec01fd8da0a454092d/scale-to-zero-endpoint).\n",
    "\n",
    "There are a few parameters we want to setup for our endpoint. We first start by setting the variant name, and instance type we want our endpoint to use. In addition we set the *model_data_download_timeout_in_seconds* and *container_startup_health_check_timeout_in_seconds* to have some guardrails for when we deploy inference components to our endpoint. In addition we will use Managed Instance Scaling which allows SageMaker to scale the number of instances based on the requirements of the scaling of your inference components. We set a *MinInstanceCount* and *MinInstanceCount* variable to size this according to the workload you want to service and also maintain controls around cost. Lastly, we set *RoutingStrategy* for the endpoint to optimally tune how to route requests to instances and inference components for the best performance.\n",
    "\n",
    "The suggested instance types to host the QwQ 30B model can be `ml.g5.24xlarge`, `ml.g6.12xlarge`, `ml.g6e.12xlarge`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6322e5d-23ad-4eb1-9d50-c88bd0ee4996",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Set an unique endpoint config name\n",
    "endpoint_config_name = f\"{prefix}-endpoint-config\"\n",
    "print(f\"Demo endpoint config name: {endpoint_config_name}\")\n",
    "\n",
    "# Set varient name and instance type for hosting\n",
    "variant_name = \"AllTraffic\"\n",
    "instance_type = \"ml.g5.24xlarge\"\n",
    "model_data_download_timeout_in_seconds = 3600\n",
    "container_startup_health_check_timeout_in_seconds = 3600\n",
    "\n",
    "min_instance_count = 0 # Minimum instance must be set to 0\n",
    "max_instance_count = 3\n",
    "\n",
    "sagemaker_client.create_endpoint_config(\n",
    "    EndpointConfigName=endpoint_config_name,\n",
    "    ExecutionRoleArn=role,\n",
    "    ProductionVariants=[\n",
    "        {\n",
    "            \"VariantName\": variant_name,\n",
    "            \"InstanceType\": instance_type,\n",
    "            \"InitialInstanceCount\": 1,\n",
    "            \"ModelDataDownloadTimeoutInSeconds\": model_data_download_timeout_in_seconds,\n",
    "            \"ContainerStartupHealthCheckTimeoutInSeconds\": container_startup_health_check_timeout_in_seconds,\n",
    "            \"ManagedInstanceScaling\": {\n",
    "                \"Status\": \"ENABLED\",\n",
    "                \"MinInstanceCount\": min_instance_count,\n",
    "                \"MaxInstanceCount\": max_instance_count,\n",
    "            },\n",
    "            \"RoutingConfig\": {\"RoutingStrategy\": \"LEAST_OUTSTANDING_REQUESTS\"},\n",
    "        }\n",
    "    ],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fe7448b-0e70-4bfa-9d39-31757c19de6f",
   "metadata": {},
   "source": [
    "### Create the SageMaker endpoint\n",
    "Next, we create our endpoint using the above endpoint config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e0182ef-7f2b-4ded-b407-a8411bd391e0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Set a unique endpoint name\n",
    "endpoint_name = f\"{prefix}-endpoint\"\n",
    "print(f\"Demo endpoint name: {endpoint_name}\")\n",
    "\n",
    "sagemaker_client.create_endpoint(\n",
    "    EndpointName=endpoint_name,\n",
    "    EndpointConfigName=endpoint_config_name,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b91c3d36-138d-4aa1-b334-ae347bd6f13f",
   "metadata": {},
   "outputs": [],
   "source": [
    "sagemaker_session.wait_for_endpoint(endpoint_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eeab9858-7545-4d63-8431-a806acc03391",
   "metadata": {},
   "source": [
    "## Deploy using Amazon SageMaker Large Model Inference (LMI) container \n",
    "In this example we are goign to use the LMI v15 container powered by vLLM 0.8.4 with support for the vLLM V1 engine. This version now supports the latest open-source models, such as Metaâ€™s Llama 4 models Scout and Maverick, Googleâ€™s Gemma 3, Alibabaâ€™s Qwen, Mistral AI, DeepSeek-R, and many more. You can find more details of the LMI v15 container from [the blog here](https://aws.amazon.com/blogs/machine-learning/supercharge-your-llm-performance-with-amazon-sagemaker-large-model-inference-container-v15/).\n",
    "\n",
    "\n",
    "\n",
    "### Create Model Artifact\n",
    "We will be deploying the Qwen 30B A3B model using the LMI container. In order to do so you need to set the image you would like to use with the proper configuartion. You can also create a SageMaker model to be referenced when you create your inference component\n",
    "\n",
    "#### Download the model from Hugging Face and upload the model artifacts on Amazon S3\n",
    "In this example, we will demonstrate how to download your copy of the model from huggingface and upload it to an s3 location in your AWS account, then deploy the model with the downloaded model artifacts to an endpoint. \n",
    "\n",
    "First, download the model artifact data from HuggingFace. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61dd4746-be89-46ad-8b5f-67203db639ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import snapshot_download\n",
    "from pathlib import Path\n",
    "import os\n",
    "import sagemaker\n",
    "import jinja2\n",
    "\n",
    "qwen3_30B = \"Qwen/Qwen3-30B-A3B\"\n",
    "\n",
    "# - This will download the model into the current directory where ever the jupyter notebook is running\n",
    "local_model_path = Path(\".\")\n",
    "local_model_path.mkdir(exist_ok=True)\n",
    "model_name = qwen3_30B\n",
    "# Only download pytorch checkpoint files\n",
    "allow_patterns = [\"*.json\", \"*.safetensors\", \"*.bin\", \"*.txt\"]\n",
    "\n",
    "# - Leverage the snapshot library to donload the model since the model is stored in repository using LFS\n",
    "model_download_path = snapshot_download(\n",
    "    repo_id=model_name,\n",
    "    cache_dir=local_model_path,\n",
    "    allow_patterns=allow_patterns,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "631ad04c-54ce-4af2-b745-b3a149a7676e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a variable to contain the s3url of the location that has the model\n",
    "pretrained_model_location = f\"s3://{model_bucket}/{s3_model_prefix}/\"\n",
    "print(f\"Pretrained model will be uploaded to ---- > {pretrained_model_location}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcd26dbd-9c82-4722-a22d-77e46ed25c1e",
   "metadata": {},
   "source": [
    "Upload model data to s3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "066f46e4-474b-40da-9fd9-5edb367af285",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_artifact = sagemaker_session.upload_data(path=model_download_path, key_prefix=s3_model_prefix)\n",
    "print(f\"Model uploaded to --- > {model_artifact}\")\n",
    "print(f\"We will set option.s3url={model_artifact}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3731b3f4-744f-46a1-b9d1-6e206cc39360",
   "metadata": {},
   "outputs": [],
   "source": [
    "# optional\n",
    "# !rm -rf {model_download_path}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba1f26a4-7591-47e4-af4e-990c59079928",
   "metadata": {},
   "source": [
    "To find our more of the SageMaker `create_model` api call, you can see the details in [the boto3 doc](https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/sagemaker/client/create_model.html). Note that you can use the **CompressionType** to specify how the model data is prepared.  \n",
    "\n",
    "If you choose `Gzip` and choose `S3Object` as the value of `S3DataType`, `S3Uri` identifies an object that is a gzip-compressed TAR archive. SageMaker will attempt to decompress and untar the object during model deployment.\n",
    "\n",
    "If you choose `None` and `S3Prefix` as the value of `S3DataType`, then for each S3 object under the key name pefix referenced by `S3Uri`, SageMaker will trim its key by the prefix, and use the remainder as the path (relative to `/opt/ml/model`) of the file holding the content of the S3 object. SageMaker will split the remainder by slash (/), using intermediate parts as directory names and the last part as filename of the file holding the content of the S3 object.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd27d45d-0624-48f2-af2f-a322e41601a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define region where you have capacity\n",
    "REGION = boto_region\n",
    "\n",
    "#Select the latest container. Check the link for the latest available version https://github.com/aws/deep-learning-containers/blob/master/available_images.md#large-model-inference-containers \n",
    "CONTAINER_VERSION = '0.33.0-lmi15.0.0-cu128'\n",
    "\n",
    "# Construct container URI\n",
    "container_uri = f'763104351884.dkr.ecr.{REGION}.amazonaws.com/djl-inference:{CONTAINER_VERSION}'\n",
    "\n",
    "pretrained_model_location = f\"s3://{model_bucket}/{s3_model_prefix}/\"\n",
    "qwen3_model = {\n",
    "    \"Image\": container_uri,\n",
    "    'ModelDataSource': {\n",
    "                'S3DataSource': {\n",
    "                    'S3Uri': pretrained_model_location,\n",
    "                    'S3DataType': 'S3Prefix',\n",
    "                    'CompressionType': 'None',\n",
    "                }\n",
    "            },\n",
    "    \"Environment\": {\n",
    "        \"SAGEMAKER_MODEL_SERVER_WORKERS\": \"1\",\n",
    "        \"MESSAGES_API_ENABLED\": \"true\",\n",
    "        \"OPTION_MAX_ROLLING_BATCH_SIZE\": \"8\",\n",
    "        \"OPTION_MODEL_LOADING_TIMEOUT\": \"1500\",\n",
    "        \"SERVING_FAIL_FAST\": \"true\",\n",
    "        \"OPTION_ROLLING_BATCH\": \"disable\",\n",
    "        \"OPTION_ASYNC_MODE\": \"true\",\n",
    "        \"OPTION_ENTRYPOINT\": \"djl_python.lmi_vllm.vllm_async_service\",\n",
    "        \"OPTION_ENABLE_STREAMING\": \"true\"\n",
    "    },\n",
    "}\n",
    "model_name_qwen3 = f\"qwen3-30b-tgi-{datetime.now().strftime('%y%m%d-%H%M%S')}\"\n",
    "# create SageMaker Model\n",
    "sagemaker_client.create_model(\n",
    "    ModelName=model_name_qwen3,\n",
    "    ExecutionRoleArn=role,\n",
    "    Containers=[qwen3_model],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e28e6094-9ea6-4481-be2e-9ee2d00b92bd",
   "metadata": {},
   "source": [
    "We can now create the Inference Components which will deployed on the endpoint that you specify. Please note here that you can provide a SageMaker model or a container to specification. If you provide a container, you will need to provide an image and artifactURL as parameters. In this example we set it to the model name we prepared in the cells above. You can also set the `ComputeResourceRequirements` to supply SageMaker what should be reserved for each copy of the inference component. You can also set the copy count of the number of Inference Components you would like to deploy. These can be managed and scaled as the capabilities become available. \n",
    "\n",
    "Note that in this example we set the `NumberOfAcceleratorDevicesRequired` to a value of `4`. By doing so we reserve 4 accelerators for each copy of this inference component so that we can use tensor parallel. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42fc8b52-e2d1-4327-b4fc-8f7d8c740bad",
   "metadata": {},
   "outputs": [],
   "source": [
    "inference_component_name_qwen = f\"{prefix}-IC-qwen3-30b-{datetime.now().strftime('%y%m%d-%H%M%S')}\"\n",
    "variant_name = \"AllTraffic\"\n",
    "\n",
    "sagemaker_client.create_inference_component(\n",
    "    InferenceComponentName=inference_component_name_qwen,\n",
    "    EndpointName=endpoint_name,\n",
    "    VariantName=variant_name,\n",
    "    Specification={\n",
    "        \"ModelName\": model_name_qwen3,\n",
    "        \"ComputeResourceRequirements\": {\n",
    "            \"NumberOfAcceleratorDevicesRequired\": 4,\n",
    "            \"NumberOfCpuCoresRequired\": 1,\n",
    "            \"MinMemoryRequiredInMb\": 1024,\n",
    "        },\n",
    "    },\n",
    "    RuntimeConfig={\"CopyCount\": 1},\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33cd376f-d3f5-4f8e-a4b5-ea11ce9fa60e",
   "metadata": {},
   "source": [
    "Wait until the inference component is `InService`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "56190add-87e3-4081-ae69-2d603911522e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating\n",
      "Creating\n",
      "Creating\n",
      "Creating\n",
      "Creating\n",
      "Creating\n",
      "Creating\n",
      "Creating\n",
      "Creating\n",
      "Creating\n",
      "Creating\n",
      "Creating\n",
      "Creating\n",
      "Creating\n",
      "Creating\n",
      "Creating\n",
      "Creating\n",
      "Creating\n",
      "Creating\n",
      "Creating\n",
      "Creating\n",
      "Creating\n",
      "Creating\n",
      "Creating\n",
      "Creating\n",
      "Creating\n",
      "Creating\n",
      "Creating\n",
      "Creating\n",
      "Creating\n",
      "Creating\n",
      "Creating\n",
      "Creating\n",
      "Creating\n",
      "Creating\n",
      "Creating\n",
      "Creating\n",
      "Creating\n",
      "InService\n",
      "\n",
      "Total time taken: 1146.90 seconds (19.11 minutes)\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "# Let's see how much it takes\n",
    "start_time = time.time()\n",
    "while True:\n",
    "    desc = sagemaker_client.describe_inference_component(\n",
    "        InferenceComponentName=inference_component_name_qwen\n",
    "    )\n",
    "    status = desc[\"InferenceComponentStatus\"]\n",
    "    print(status)\n",
    "    sys.stdout.flush()\n",
    "    if status in [\"InService\", \"Failed\"]:\n",
    "        break\n",
    "    time.sleep(30)\n",
    "total_time = time.time() - start_time\n",
    "print(f\"\\nTotal time taken: {total_time:.2f} seconds ({total_time/60:.2f} minutes)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "865e285c-b98c-4e23-87dd-739898c32dec",
   "metadata": {},
   "source": [
    "#### Invoke endpoint with boto3\n",
    "Now you can invoke the endpoint with boto3 `invoke_endpoint` or `invoke_endpoint_with_response_stream` runtime api calls. If you have an existing endpoint, you don't need to recreate the `predictor` and can follow below example to invoke the endpoint with an endpoint name.\n",
    "\n",
    "Note that based on the [Qwen3 hugging face page description](https://huggingface.co/Qwen/Qwen3-30B-A3B), by default, Qwen3 has thinking capabilities enabled, similar to QwQ-32B. This means the model will use its reasoning abilities to enhance the quality of generated responses. In this mode, the model will generate think content wrapped in a \\<think\\>...\\</think\\> block, followed by the final response. For thinking mode, use Temperature=0.6, TopP=0.95, TopK=20, and MinP=0 (the default setting in generation_config.json).\n",
    "\n",
    "It also allows a hard switch to strictly disable the model's thinking behavior, aligning its functionality with the previous Qwen2.5-Instruct models. This mode is particularly useful in scenarios where disabling thinking is essential for enhancing efficiency. For non-thinking mode, we suggest using Temperature=0.7, TopP=0.8, TopK=20, and MinP=0.\n",
    "\n",
    "**Advanced Usage**: You can also switch Between `Thinking` and `Non-Thinking` Modes via User Input\n",
    "Qwen3 provides a soft switch mechanism that allows users to dynamically control the model's behavior when `enable_thinking=True`. Specifically, you can add `/think` and `/no_think` to user prompts or system messages to switch the model's thinking mode from turn to turn. The model will follow the most recent instruction in multi-turn conversations.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "6817557b-8204-4171-bcaa-54395f8b8868",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<think>\n",
      "Okay, let's see. The question is asking how many R's are in the word \"STRAWBERRY.\" Hmm, I need to count the letters R in that word.\n",
      "\n",
      "First, I should write out the word to make sure I have it right: S-T-R-A-W-B-E-R-R-Y. Let me check each letter one by one. \n",
      "\n",
      "Starting with S â€“ that's not an R. Then T â€“ nope. Next is R. That's one R. Then A, W, B, E. Then the next letter is R again. That's the second R. Then another R. Wait, so after E, it's R, R, Y. So that's two more R's? Wait, let me go through it again step by step.\n",
      "\n",
      "Breaking down the word:\n",
      "\n",
      "1. S\n",
      "2. T\n",
      "3. R\n",
      "4. A\n",
      "5. W\n",
      "6. B\n",
      "7. E\n",
      "8. R\n",
      "9. R\n",
      "10. Y\n",
      "\n",
      "So positions 3, 8, and 9 are R's. That's three R's. Wait, but sometimes people might miscount. Let me check again. The word is S-T-R-A-W-B-E-R-R-Y. So after the E, there's R, R, Y. So that's two R's there. Plus the R in the third position. So total of three R's. \n",
      "\n",
      "But wait, maybe I'm miscounting. Let me spell it out again: S-T-R-A-W-B-E-R-R-Y. So the letters are S, T, R, A, W, B, E, R, R, Y. So yes, R is at position 3, 8, and 9. That's three R's. So the answer should be 3. But I should make sure I didn't miss any. Sometimes people might overlook letters if they're not careful. Let me write it again:\n",
      "\n",
      "S T R A W B E R R Y\n",
      "\n",
      "Counting each R:\n",
      "\n",
      "- The third letter is R (1)\n",
      "- The eighth letter is R (2)\n",
      "- The ninth letter is R (3)\n",
      "- The tenth is Y, so that's it.\n",
      "\n",
      "Yes, three R's. So the answer is 3.\n",
      "</think>\n",
      "\n",
      "There are 3 R's in \"STRAWBERRY\".  \n",
      "**Explanation**: The word is S-T-R-A-W-B-E-R-R-Y. The letters R appear at positions \n"
     ]
    }
   ],
   "source": [
    "import boto3\n",
    "import json\n",
    "sagemaker_runtime = boto3.client('sagemaker-runtime')\n",
    "\n",
    "prompt = {\n",
    "    'messages':[\n",
    "    {\"role\": \"user\", \"content\": \"How many R are in STRAWBERRY? Keep your answer and explanation short!\"}\n",
    "],\n",
    "    'temperature':0.7,\n",
    "    'top_p':0.8,\n",
    "    'top_k':20,\n",
    "    'max_tokens':512,\n",
    "}\n",
    "response = sagemaker_runtime.invoke_endpoint(\n",
    "    EndpointName=endpoint_name,\n",
    "    InferenceComponentName=inference_component_name_qwen,\n",
    "    ContentType=\"application/json\",\n",
    "    Body=json.dumps(prompt)\n",
    ")\n",
    "response_dict = json.loads(response['Body'].read().decode(\"utf-8\"))\n",
    "response_content = response_dict['choices'][0]['message']['content']\n",
    "print(response_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "0515e725-c8ee-4ae2-875c-1252ebaab49f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<think>\n",
      "\n",
      "</think>\n",
      "\n",
      "There is **1 R** in \"STRAWBERRY\".\n"
     ]
    }
   ],
   "source": [
    "# Soft switch to no thinking\n",
    "prompt = {\n",
    "    'messages':[\n",
    "    {\"role\": \"user\", \"content\": \"How many R are in STRAWBERRY? Keep your answer and explanation short! /no_think\"}\n",
    "],\n",
    "    'temperature':0.7,\n",
    "    'top_p':0.8,\n",
    "    'top_k':20,\n",
    "    'max_tokens':512,\n",
    "}\n",
    "response = sagemaker_runtime.invoke_endpoint(\n",
    "    EndpointName=endpoint_name,\n",
    "    InferenceComponentName=inference_component_name_qwen,\n",
    "    ContentType=\"application/json\",\n",
    "    Body=json.dumps(prompt)\n",
    ")\n",
    "response_dict = json.loads(response['Body'].read().decode(\"utf-8\"))\n",
    "response_content = response_dict['choices'][0]['message']['content']\n",
    "print(response_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "015339db-9d8c-481b-bca9-5f7955991a5b",
   "metadata": {},
   "source": [
    "#### Streaming response from the endpoint\n",
    "Additionally, SGLang allows you to invoke the endpoint and receive streaming response. Below is an example of how to interact with the endpoint with streaming response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "a65f8ce7-fcf1-4276-a1ae-98dc3ef7638b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import io\n",
    "import json\n",
    "\n",
    "# Example class that processes an inference stream:\n",
    "class SmrInferenceStream:\n",
    "    \n",
    "    def __init__(self, sagemaker_runtime, endpoint_name, inference_component_name=None):\n",
    "        self.sagemaker_runtime = sagemaker_runtime\n",
    "        self.endpoint_name = endpoint_name\n",
    "        self.inference_component_name = inference_component_name\n",
    "        # A buffered I/O stream to combine the payload parts:\n",
    "        self.buff = io.BytesIO() \n",
    "        self.read_pos = 0\n",
    "        \n",
    "    def stream_inference(self, request_body):\n",
    "        # Gets a streaming inference response \n",
    "        # from the specified model endpoint:\n",
    "        response = self.sagemaker_runtime\\\n",
    "            .invoke_endpoint_with_response_stream(\n",
    "                EndpointName=self.endpoint_name, \n",
    "                InferenceComponentName=self.inference_component_name,\n",
    "                Body=json.dumps(request_body), \n",
    "                ContentType=\"application/json\"\n",
    "        )\n",
    "        # Gets the EventStream object returned by the SDK:\n",
    "        event_stream = response['Body']\n",
    "        for event in event_stream:\n",
    "            # Passes the contents of each payload part\n",
    "            # to be concatenated:\n",
    "            self._write(event['PayloadPart']['Bytes'])\n",
    "            # Iterates over lines to parse whole JSON objects:\n",
    "            for line in self._readlines():\n",
    "                try:\n",
    "                    resp = json.loads(line)\n",
    "                except:\n",
    "                    continue\n",
    "                if len(line)>0 and type(resp) == dict:\n",
    "                    # if len(resp.get('choices')) == 0:\n",
    "                    #     continue\n",
    "                    part = resp.get('choices')[0]['delta']['content']\n",
    "                    \n",
    "                else:\n",
    "                    part = resp\n",
    "                # Returns parts incrementally:\n",
    "                yield part\n",
    "    \n",
    "    # Writes to the buffer to concatenate the contents of the parts:\n",
    "    def _write(self, content):\n",
    "        self.buff.seek(0, io.SEEK_END)\n",
    "        self.buff.write(content)\n",
    "\n",
    "    # The JSON objects in buffer end with '\\n'.\n",
    "    # This method reads lines to yield a series of JSON objects:\n",
    "    def _readlines(self):\n",
    "        self.buff.seek(self.read_pos)\n",
    "        for line in self.buff.readlines():\n",
    "            self.read_pos += len(line)\n",
    "            yield line[:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "40f29fdb-e4be-439c-a6e4-dc05facc6b92",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<think>\n",
      "Okay, let's see. The question is asking how many R's are in the word \"STRAWBERRY.\" Alright, first I need to spell out the word correctly. Let me write it down: S-T-R-A-W-B-E-R-R-Y. Wait, is that right? Let me check again. S-T-R-A-W-B-E-R-R-Y. Yeah, that's how it's spelled. Now, I need to count the letter R in there.\n",
      "\n",
      "Let me go through each letter one by one. Starting with S â€“ that's not an R. Then T â€“ nope. Next is R. That's one R. Then A, W, B, E. None of those are R. Then the next letter is R again. So that's two R's. Then another R? Wait, after E comes R, then another R, and then Y. So let me break it down:\n",
      "\n",
      "1. S\n",
      "2. T\n",
      "3. R (1)\n",
      "4. A\n",
      "5. W\n",
      "6. B\n",
      "7. E\n",
      "8. R (2)\n",
      "9. R (3)\n",
      "10. Y\n",
      "\n",
      "Wait, so after E, there are two R's? Let me check the spelling again. STRAWBERRY. The correct spelling is S-T-R-A-W-B-E-R-R-Y. So yes, after the E, there are two R's before the Y. So that's three R's in total? Wait, no. Let me count again:\n",
      "\n",
      "S (1), T (2), R (3), A (4), W (5), B (6), E (7), R (8), R (9), Y (10). So positions 3, 8, and 9 are R's. So that's three R's. But wait, sometimes people might miss that. Let me make sure. The word is spelled S-T-R-A-W-B-E-R-R-Y. So the R's are in the third position, then after the E, there are two more R's. So total of three R's. But wait, I thought maybe there's only two. Maybe I'm miscounting. Let me write it out:\n",
      "\n",
      "S T R A W B E R R Y\n",
      "\n",
      "Breaking it down:\n",
      "\n",
      "1. S\n",
      "2. T\n",
      "3. R\n",
      "4. A\n",
      "5. W\n",
      "6. B\n",
      "7. E\n",
      "8. R\n",
      "9. R\n",
      "10. Y\n",
      "\n",
      "Yes"
     ]
    }
   ],
   "source": [
    "request_body = {\n",
    "    'messages':[\n",
    "        {\"role\": \"user\", \"content\": \"How many R are in STRAWBERRY? Keep your answer and explanation short!\"},\n",
    "    ],\n",
    "    'temperature':0.9,\n",
    "    'max_tokens':512,\n",
    "    'stream': True,\n",
    "}\n",
    "\n",
    "smr_inference_stream = SmrInferenceStream(\n",
    "    sagemaker_runtime, endpoint_name, inference_component_name_qwen)\n",
    "stream = smr_inference_stream.stream_inference(request_body)\n",
    "for part in stream:\n",
    "    print(part, end='')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bacb334-1383-4bab-a118-683d3328b929",
   "metadata": {},
   "source": [
    "## Cleanup\n",
    "  \n",
    "Make sure to delete the endpoint and other artifacts that were created to avoid unnecessary cost. You can also go to SageMaker AI console to delete all the resources created in this example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a71881f-1458-4557-9a13-9d764377e44e",
   "metadata": {},
   "outputs": [],
   "source": [
    "sagemaker_client.delete_inference_component(InferenceComponentName=inference_component_name_qwen)\n",
    "sagemaker_client.delete_endpoint(EndpointName=endpoint_name)\n",
    "sagemaker_client.delete_endpoint_config(EndpointConfigName=endpoint_config_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "789f12ce-8c71-47c6-a741-2b1286d68fe4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
